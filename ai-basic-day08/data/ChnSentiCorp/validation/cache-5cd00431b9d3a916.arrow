 "learning_rate": 2.073498140371899e-06,
      "loss": 0.0101,
      "step": 1565
    },
    {
      "epoch": 13.029045643153527,
      "grad_norm": 0.13859069347381592,
      "learning_rate": 1.9873786636889906e-06,
      "loss": 0.0113,
      "step": 1570
    },
    {
      "epoch": 13.070539419087137,
      "grad_norm": 1.2401254177093506,
      "learning_rate": 1.9030116872178316e-06,
      "loss": 0.0134,
      "step": 1575
    },
    {
      "epoch": 13.112033195020746,
      "grad_norm": 1.2529823780059814,
      "learning_rate": 1.8204036358303173e-06,
      "loss": 0.0085,
      "step": 1580
    },
    {
      "epoch": 13.153526970954356,
      "grad_norm": 0.487405925989151,
      "learning_rate": 1.7395608004493886e-06,
      "loss": 0.008,
      "step": 1585
    },
    {
      "epoch": 13.195020746887966,
      "grad_norm": 0.2875262200832367,
      "learning_rate": 1.6604893375699594e-06,
      "loss": 0.0102,
      "step": 1590
    },
    {
      "epoch": 13.236514522821576,
      "grad_norm": 0.7105116248130798,
      "learning_rate": 1.5831952687900608e-06,
      "loss": 0.0108,
      "step": 1595
    },
    {
      "epoch": 13.278008298755188,
      "grad_norm": 0.9724109768867493,
      "learning_rate": 1.5076844803522922e-06,
      "loss": 0.0132,
      "step": 1600
    },
    {
      "epoch": 13.278008298755188,
      "eval_loss": 2.4680614471435547,
      "eval_runtime": 1.1169,
      "eval_samples_per_second": 8.953,
      "eval_steps_per_second": 4.477,
      "step": 1600
    },
    {
      "epoch": 13.319502074688797,
      "grad_norm": 0.28990522027015686,
      "learning_rate": 1.4339627226955392e-06,
      "loss": 0.0075,
      "step": 1605
    },
    {
      "epoch": 13.360995850622407,
      "grad_norm": 0.32019373774528503,
      "learning_rate": 1.362035610017079e-06,
      "loss": 0.008,
      "step": 1610
    },
    {
      "epoch": 13.402489626556017,
      "grad_norm": 0.38221970200538635,
      "learning_rate": 1.291908619845017e-06,
      "loss": 0.0082,
      "step": 1615
    },
    {
      "epoch": 13.443983402489627,
      "grad_norm": 0.4838140606880188,
      "learning_rate": 1.2235870926211619e-06,
      "loss": 0.0108,
      "step": 1620
    },
    {
      "epoch": 13.485477178423237,
      "grad_norm": 0.2922254204750061,
      "learning_rate": 1.1570762312943295e-06,
      "loss": 0.0086,
      "step": 1625
    },
    {
      "epoch": 13.526970954356846,
      "grad_norm": 0.44325631856918335,
      "learning_rate": 1.0923811009241142e-06,
      "loss": 0.0091,
      "step": 1630
    },
    {
      "epoch": 13.568464730290456,
      "grad_norm": 0.27134034037590027,
      "learning_rate": 1.0295066282951738e-06,
      "loss": 0.0056,
      "step": 1635
    },
    {
      "epoch": 13.609958506224066,
      "grad_norm": 0.36983051896095276,
      "learning_rate": 9.684576015420278e-07,
      "loss": 0.0189,
      "step": 1640
    },
    {
      "epoch": 13.651452282157676,
      "grad_norm": 0.3034515380859375,
      "learning_rate": 9.092386697844263e-07,
      "loss": 0.0054,
      "step": 1645
    },
    {
      "epoch": 13.692946058091286,
      "grad_norm": 0.8970560431480408,
      "learning_rate": 8.51854342773295e-07,
      "loss": 0.0144,
      "step": 1650
    },
    {
      "epoch": 13.734439834024897,
      "grad_norm": 0.2997713088989258,
      "learning_rate": 7.963089905473092e-07,
      "loss": 0.0072,
      "step": 1655
    },
    {
      "epoch": 13.775933609958507,
      "grad_norm": 0.17190326750278473,
      "learning_rate": 7.426068431000882e-07,
      "loss": 0.0202,
      "step": 1660
    },
    {
      "epoch": 13.817427385892117,
      "grad_norm": 0.8538981080055237,
      "learning_rate": 6.907519900580861e-07,
      "loss": 0.0129,
      "step": 1665
    },
    {
      "epoch": 13.858921161825727,
      "grad_norm": 0.2791764736175537,
      "learning_rate": 6.407483803691216e-07,
      "loss": 0.012,
      "step": 1670
    },
    {
      "epoch": 13.900414937759336,
      "grad_norm": 0.24045471847057343,
      "learning_rate": 5.925998220016659e-07,
      "loss": 0.0131,
      "step": 1675
    },
    {
      "epoch": 13.941908713692946,
      "grad_norm": 0.3419439494609833,
      "learning_rate": 5.463099816548579e-07,
      "loss": 0.0061,
      "step": 1680
    },
    {
      "epoch": 13.983402489626556,
      "grad_norm": 0.4937282204627991,
      "learning_rate": 5.018823844792603e-07,
      "loss": 0.0102,
      "step": 1685
    },
    {
      "epoch": 14.024896265560166,
      "grad_norm": 0.33260440826416016,
      "learning_rate": 4.5932041380840065e-07,
      "loss": 0.0069,
      "step": 1690
    },
    {
      "epoch": 14.066390041493776,
      "grad_norm": 0.7790851593017578,
      "learning_rate": 4.1862731090113736e-07,
      "loss": 0.0109,
      "step": 1695
    },
    {
      "epoch": 14.107883817427386,
      "grad_norm": 0.3601318299770355,
      "learning_rate": 3.7980617469479953e-07,
      "loss": 0.0049,
      "step": 1700
    },
    {
      "epoch": 14.107883817427386,
      "eval_loss": 2.4944987297058105,
      "eval_runtime": 1.1151,
      "eval_samples_per_second": 8.968,
      "eval_steps_per_second": 4.484,
      "step": 1700
    },
    {
      "epoch": 14.149377593360995,
      "grad_norm": 0.26043277978897095,
      "learning_rate": 3.428599615692141e-07,
      "loss": 0.0151,
      "step": 1705
    },
    {
      "epoch": 14.190871369294605,
      "grad_norm": 0.43970608711242676,
      "learning_rate": 3.077914851215585e-07,
      "loss": 0.0068,
      "step": 1710
    },
    {
      "epoch": 14.232365145228215,
      "grad_norm": 0.3800696134567261,
      "learning_rate": 2.746034159520794e-07,
      "loss": 0.0172,
      "step": 1715
    },
    {
      "epoch": 14.273858921161827,
      "grad_norm": 0.2721009850502014,
      "learning_rate": 2.4329828146074095e-07,
      "loss": 0.0084,
      "step": 1720
    },
    {
      "epoch": 14.315352697095436,
      "grad_norm": 0.22687344253063202,
      "learning_rate": 2.1387846565474045e-07,
      "loss": 0.0066,
      "step": 1725
    },
    {
      "epoch": 14.356846473029046,
      "grad_norm": 0.49835205078125,
      "learning_rate": 1.8634620896695043e-07,
      "loss": 0.0096,
      "step": 1730
    },
    {
      "epoch": 14.398340248962656,
      "grad_norm": 0.4317694902420044,
      "learning_rate": 1.607036080853136e-07,
      "loss": 0.0061,
      "step": 1735
    },
    {
      "epoch": 14.439834024896266,
      "grad_norm": 0.34637778997421265,
      "learning_rate": 1.3695261579316777e-07,
      "loss": 0.0124,
      "step": 1740
    },
    {
      "epoch": 14.481327800829876,
      "grad_norm": 0.31880512833595276,
      "learning_rate": 1.1509504082052869e-07,
      "loss": 0.0102,
      "step": 1745
    },
    {
      "epoch": 14.522821576763485,
      "grad_norm": 0.2113306075334549,
      "learning_rate": 9.513254770636137e-08,
      "loss": 0.0087,
      "step": 1750
    },
    {
      "epoch": 14.564315352697095,
      "grad_norm": 0.4641111493110657,
      "learning_rate": 7.706665667180091e-08,
      "loss": 0.0117,
      "step": 1755
    },
    {
      "epoch": 14.605809128630705,
      "grad_norm": 1.240736961364746,
      "learning_rate": 6.089874350439506e-08,
      "loss": 0.0089,
      "step": 1760
    },
    {
      "epoch": 14.647302904564315,
      "grad_norm": 0.38648852705955505,
      "learning_rate": 4.6630039453327e-08,
      "loss": 0.0129,
      "step": 1765
    },
    {
      "epoch": 14.688796680497925,
      "grad_norm": 0.35339808464050293,
      "learning_rate": 3.426163113565417e-08,
      "loss": 0.0072,
      "step": 1770
    },
    {
      "epoch": 14.730290456431534,
      "grad_norm": 0.32902586460113525,
      "learning_rate": 2.3794460453555047e-08,
      "loss": 0.0095,
      "step": 1775
    },
    {
      "epoch": 14.771784232365146,
      "grad_norm": 0.10452105849981308,
      "learning_rate": 1.522932452260595e-08,
      "loss": 0.0076,
      "step": 1780
    },
    {
      "epoch": 14.813278008298756,
      "grad_norm": 1.3786877393722534,
      "learning_rate": 8.566875611068504e-09,
      "loss": 0.0069,
      "step": 1785
    },
    {
      "epoch": 14.854771784232366,
      "grad_norm": 0.690101146697998,
      "learning_rate": 3.807621090218261e-09,
      "loss": 0.0104,
      "step": 1790
    },
    {
      "epoch": 14.896265560165975,
      "grad_norm": 0.4032169282436371,
      "learning_rate": 9.51923395717258e-10,
      "loss": 0.0072,
      "step": 1795
    },
    {
      "epoch": 14.937759336099585,
      "grad_norm": 0.6054149270057678,
      "learning_rate": 0.0,
      "loss": 0.0117,
      "step": 1800
    },
    {
      "epoch": 14.937759336099585,
      "eval_loss": 2.504978656768799,
      "eval_runtime": 1.1153,
      "eval_samples_per_second": 8.967,
      "eval_steps_per_second": 4.483,
      "step": 1800
    }
  ],
  "logging_steps": 5,
  "max_steps": 1800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9591653795692544e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
                                                                                                                                                                                                                                                                                                                                                                                    