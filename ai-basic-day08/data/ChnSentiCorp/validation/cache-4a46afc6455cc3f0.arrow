7.468879668049793,
      "grad_norm": 3.3089418411254883,
      "learning_rate": 2.5e-05,
      "loss": 0.1056,
      "step": 900
    },
    {
      "epoch": 7.468879668049793,
      "eval_loss": 1.8969625234603882,
      "eval_runtime": 1.1184,
      "eval_samples_per_second": 8.942,
      "eval_steps_per_second": 4.471,
      "step": 900
    },
    {
      "epoch": 7.5103734439834025,
      "grad_norm": 3.186399459838867,
      "learning_rate": 2.4781836612540657e-05,
      "loss": 0.1229,
      "step": 905
    },
    {
      "epoch": 7.551867219917012,
      "grad_norm": 2.4051709175109863,
      "learning_rate": 2.4563689839067913e-05,
      "loss": 0.0818,
      "step": 910
    },
    {
      "epoch": 7.593360995850622,
      "grad_norm": 2.6251847743988037,
      "learning_rate": 2.4345576292303176e-05,
      "loss": 0.1116,
      "step": 915
    },
    {
      "epoch": 7.634854771784232,
      "grad_norm": 4.400695323944092,
      "learning_rate": 2.4127512582437485e-05,
      "loss": 0.1029,
      "step": 920
    },
    {
      "epoch": 7.676348547717843,
      "grad_norm": 0.8855239152908325,
      "learning_rate": 2.3909515315866605e-05,
      "loss": 0.0694,
      "step": 925
    },
    {
      "epoch": 7.717842323651452,
      "grad_norm": 2.7722060680389404,
      "learning_rate": 2.3691601093926404e-05,
      "loss": 0.1056,
      "step": 930
    },
    {
      "epoch": 7.759336099585062,
      "grad_norm": 6.435791492462158,
      "learning_rate": 2.3473786511628575e-05,
      "loss": 0.1593,
      "step": 935
    },
    {
      "epoch": 7.800829875518672,
      "grad_norm": 4.933192253112793,
      "learning_rate": 2.3256088156396868e-05,
      "loss": 0.1363,
      "step": 940
    },
    {
      "epoch": 7.842323651452282,
      "grad_norm": 3.067148208618164,
      "learning_rate": 2.303852260680388e-05,
      "loss": 0.094,
      "step": 945
    },
    {
      "epoch": 7.8838174273858925,
      "grad_norm": 7.707957744598389,
      "learning_rate": 2.2821106431308544e-05,
      "loss": 0.1444,
      "step": 950
    },
    {
      "epoch": 7.925311203319502,
      "grad_norm": 5.187991142272949,
      "learning_rate": 2.26038561869944e-05,
      "loss": 0.1278,
      "step": 955
    },
    {
      "epoch": 7.966804979253112,
      "grad_norm": 1.7858970165252686,
      "learning_rate": 2.238678841830867e-05,
      "loss": 0.0953,
      "step": 960
    },
    {
      "epoch": 8.008298755186722,
      "grad_norm": 1.3154903650283813,
      "learning_rate": 2.2169919655802335e-05,
      "loss": 0.1115,
      "step": 965
    },
    {
      "epoch": 8.049792531120332,
      "grad_norm": 2.5484910011291504,
      "learning_rate": 2.195326641487132e-05,
      "loss": 0.0653,
      "step": 970
    },
    {
      "epoch": 8.091286307053942,
      "grad_norm": 0.9102132320404053,
      "learning_rate": 2.173684519449872e-05,
      "loss": 0.0541,
      "step": 975
    },
    {
      "epoch": 8.132780082987551,
      "grad_norm": 4.086490631103516,
      "learning_rate": 2.1520672475998373e-05,
      "loss": 0.0767,
      "step": 980
    },
    {
      "epoch": 8.174273858921161,
      "grad_norm": 3.37322998046875,
      "learning_rate": 2.1304764721759733e-05,
      "loss": 0.0581,
      "step": 985
    },
    {
      "epoch": 8.215767634854771,
      "grad_norm": 2.0912463665008545,
      "learning_rate": 2.1089138373994223e-05,
      "loss": 0.0586,
      "step": 990
    },
    {
      "epoch": 8.25726141078838,
      "grad_norm": 1.772748589515686,
      "learning_rate": 2.087380985348306e-05,
      "loss": 0.0574,
      "step": 995
    },
    {
      "epoch": 8.298755186721992,
      "grad_norm": 2.2666165828704834,
      "learning_rate": 2.0658795558326743e-05,
      "loss": 0.0373,
      "step": 1000
    },
    {
      "epoch": 8.298755186721992,
      "eval_loss": 2.0173425674438477,
      "eval_runtime": 1.1176,
      "eval_samples_per_second": 8.947,
      "eval_steps_per_second": 4.474,
      "step": 1000
    },
    {
      "epoch": 8.340248962655602,
      "grad_norm": 2.8164243698120117,
      "learning_rate": 2.0444111862696314e-05,
      "loss": 0.057,
      "step": 1005
    },
    {
      "epoch": 8.381742738589212,
      "grad_norm": 2.2777435779571533,
      "learning_rate": 2.022977511558638e-05,
      "loss": 0.0493,
      "step": 1010
    },
    {
      "epoch": 8.423236514522822,
      "grad_norm": 2.3078343868255615,
      "learning_rate": 2.0015801639570074e-05,
      "loss": 0.0263,
      "step": 1015
    },
    {
      "epoch": 8.464730290456432,
      "grad_norm": 0.5701624155044556,
      "learning_rate": 1.980220772955602e-05,
      "loss": 0.0551,
      "step": 1020
    },
    {
      "epoch": 8.506224066390041,
      "grad_norm": 2.1641955375671387,
      "learning_rate": 1.958900965154743e-05,
      "loss": 0.044,
      "step": 1025
    },
    {
      "epoch": 8.547717842323651,
      "grad_norm": 3.5694987773895264,
      "learning_rate": 1.937622364140338e-05,
      "loss": 0.1452,
      "step": 1030
    },
    {
      "epoch": 8.589211618257261,
      "grad_norm": 2.8435378074645996,
      "learning_rate": 1.9163865903602374e-05,
      "loss": 0.0582,
      "step": 1035
    },
    {
      "epoch": 8.630705394190871,
      "grad_norm": 2.451082706451416,
      "learning_rate": 1.895195261000831e-05,
      "loss": 0.1002,
      "step": 1040
    },
    {
      "epoch": 8.67219917012448,
      "grad_norm": 0.8909063339233398,
      "learning_rate": 1.874049989863896e-05,
      "loss": 0.0748,
      "step": 1045
    },
    {
      "epoch": 8.71369294605809,
      "grad_norm": 3.173391819000244,
      "learning_rate": 1.852952387243698e-05,
      "loss": 0.074,
      "step": 1050
    },
    {
      "epoch": 8.755186721991702,
      "grad_norm": 2.4734833240509033,
      "learning_rate": 1.831904059804358e-05,
      "loss": 0.0554,
      "step": 1055
    },
    {
      "epoch": 8.796680497925312,
      "grad_norm": 3.2590808868408203,
      "learning_rate": 1.8109066104575023e-05,
      "loss": 0.0651,
      "step": 1060
    },
    {
      "epoch": 8.838174273858922,
      "grad_norm": 1.6505866050720215,
      "learning_rate": 1.7899616382401936e-05,
      "loss": 0.0928,
      "step": 1065
    },
    {
      "epoch": 8.879668049792532,
      "grad_norm": 2.54710054397583,
      "learning_rate": 1.7690707381931583e-05,
      "loss": 0.051,
      "step": 1070
    },
    {
      "epoch": 8.921161825726141,
      "grad_norm": 1.5605746507644653,
      "learning_rate": 1.7482355012393177e-05,
      "loss": 0.0517,
      "step": 1075
    },
    {
      "epoch": 8.962655601659751,
      "grad_norm": 1.611121654510498,
      "learning_rate": 1.7274575140626318e-05,
      "loss": 0.0438,
      "step": 1080
    },
    {
      "epoch": 9.004149377593361,
      "grad_norm": 3.823546886444092,
      "learning_rate": 1.7067383589872703e-05,
      "loss": 0.0822,
      "step": 1085
    },
    {
      "epoch": 9.04564315352697,
      "grad_norm": 1.5240107774734497,
      "learning_rate": 1.686079613857109e-05,
      "loss": 0.0287,
      "step": 1090
    },
    {
      "epoch": 9.08713692946058,
      "grad_norm": 2.7562007904052734,
      "learning_rate": 1.665482851915573e-05,
      "loss": 0.0593,
      "step": 1095
    },
    {
      "epoch": 9.12863070539419,
      "grad_norm": 0.6180751919746399,
      "learning_rate": 1.6449496416858284e-05,
      "loss": 0.0517,
      "step": 1100
    },
    {
      "epoch": 9.12863070539419,
      "eval_loss": 2.1241869926452637,
      "eval_runtime": 1.1181,
      "eval_samples_per_second": 8.944,
      "eval_steps_per_second": 4.472,
      "step": 1100
    },
    {
      "epoch": 9.1701244813278,
      "grad_norm": 6.051784038543701,
      "learning_rate": 1.6244815468513315e-05,
      "loss": 0.0606,
      "step": 1105
    },
    {
      "epoch": 9.21161825726141,
      "grad_norm": 1.9720449447631836,
      "learning_rate": 1.6040801261367493e-05,
      "loss": 0.0343,
      "step": 1110
    },
    {
      "epoch": 9.25311203319502,
      "grad_norm": 2.249544620513916,
      "learning_rate": 1.583746933189257e-05,
      "loss": 0.0439,
      "step": 1115
    },
    {
      "epoch": 9.294605809128631,
      "grad_norm": 2.8268909454345703,
      "learning_rate": 1.56348351646022e-05,
      "loss": 0.0279,
      "step": 1120
    },
    {
      "epoch": 9.336099585062241,
      "grad_norm": 1.0213853120803833,
      "learning_rate": 1.5432914190872757e-05,
      "loss": 0.0287,
      "step": 1125
    },
    {
      "epoch": 9.377593360995851,
      "grad_norm": 0.3406469523906708,
      "learning_rate": 1.523172178776816e-05,
      "loss": 0.0454,
      "step": 1130
    },
    {
      "epoch": 9.41908713692946,
      "grad_norm": 0.20024168491363525,
      "learning_rate": 1.5031273276868845e-05,
      "loss": 0.0211,
      "step": 1135
    },
    {
      "epoch": 9.46058091286307,
      "grad_norm": 1.2807449102401733,
      "learning_rate": 1.4831583923104999e-05,
      "loss": 0.0379,
      "step": 1140
    },
    {
      "epoch": 9.50207468879668,
      "grad_norm": 3.30271053314209,
      "learning_rate": 1.463266893359403e-05,
      "loss": 0.0261,
      "step": 1145
    },
    {
      "epoch": 9.54356846473029,
      "grad_norm": 0.8610305190086365,
      "learning_rate": 1.443454345648252e-05,
      "loss": 0.0259,
      "step": 1150
    },
    {
      "epoch": 9.5850622406639,
      "grad_norm": 2.36480450630188,
      "learning_rate": 1.4237222579792618e-05,
      "loss": 0.0645,
      "step": 1155
    },
    {
      "epoch": 9.62655601659751,
      "grad_norm": 1.5168051719665527,
      "learning_rate": 1.4040721330273062e-05,
      "loss": 0.0386,
      "step": 1160
    },
    {
      "epoch": 9.66804979253112,
      "grad_norm": 1.74