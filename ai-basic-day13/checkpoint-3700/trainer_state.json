{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 360.9756097560976,
  "eval_steps": 100,
  "global_step": 3700,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 0.9751888513565063,
      "learning_rate": 4.999996915749259e-05,
      "loss": 2.005,
      "step": 5
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 0.5990287661552429,
      "learning_rate": 4.999987663004646e-05,
      "loss": 1.8718,
      "step": 10
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 0.49335816502571106,
      "learning_rate": 4.999972241788991e-05,
      "loss": 2.059,
      "step": 15
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 0.44874903559684753,
      "learning_rate": 4.999950652140343e-05,
      "loss": 1.7514,
      "step": 20
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 0.44701236486434937,
      "learning_rate": 4.9999228941119745e-05,
      "loss": 2.0086,
      "step": 25
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 0.39112234115600586,
      "learning_rate": 4.999888967772375e-05,
      "loss": 1.6574,
      "step": 30
    },
    {
      "epoch": 3.4146341463414633,
      "grad_norm": 0.3756323754787445,
      "learning_rate": 4.999848873205254e-05,
      "loss": 1.8653,
      "step": 35
    },
    {
      "epoch": 3.902439024390244,
      "grad_norm": 0.38141176104545593,
      "learning_rate": 4.9998026105095405e-05,
      "loss": 1.6703,
      "step": 40
    },
    {
      "epoch": 4.390243902439025,
      "grad_norm": 0.34893208742141724,
      "learning_rate": 4.999750179799384e-05,
      "loss": 1.8396,
      "step": 45
    },
    {
      "epoch": 4.878048780487805,
      "grad_norm": 0.3641126751899719,
      "learning_rate": 4.999691581204152e-05,
      "loss": 1.5988,
      "step": 50
    },
    {
      "epoch": 5.365853658536586,
      "grad_norm": 0.37366774678230286,
      "learning_rate": 4.99962681486843e-05,
      "loss": 1.8001,
      "step": 55
    },
    {
      "epoch": 5.853658536585366,
      "grad_norm": 0.362895131111145,
      "learning_rate": 4.999555880952023e-05,
      "loss": 1.5829,
      "step": 60
    },
    {
      "epoch": 6.341463414634147,
      "grad_norm": 0.38752472400665283,
      "learning_rate": 4.999478779629953e-05,
      "loss": 1.8118,
      "step": 65
    },
    {
      "epoch": 6.829268292682927,
      "grad_norm": 0.4032873809337616,
      "learning_rate": 4.999395511092461e-05,
      "loss": 1.5424,
      "step": 70
    },
    {
      "epoch": 7.317073170731708,
      "grad_norm": 0.4774037301540375,
      "learning_rate": 4.9993060755450015e-05,
      "loss": 1.7716,
      "step": 75
    },
    {
      "epoch": 7.804878048780488,
      "grad_norm": 0.4360498785972595,
      "learning_rate": 4.99921047320825e-05,
      "loss": 1.5215,
      "step": 80
    },
    {
      "epoch": 8.292682926829269,
      "grad_norm": 0.4358437657356262,
      "learning_rate": 4.999108704318095e-05,
      "loss": 1.7138,
      "step": 85
    },
    {
      "epoch": 8.78048780487805,
      "grad_norm": 0.4980611801147461,
      "learning_rate": 4.999000769125641e-05,
      "loss": 1.4949,
      "step": 90
    },
    {
      "epoch": 9.268292682926829,
      "grad_norm": 0.47248736023902893,
      "learning_rate": 4.998886667897208e-05,
      "loss": 1.705,
      "step": 95
    },
    {
      "epoch": 9.75609756097561,
      "grad_norm": 0.4759368300437927,
      "learning_rate": 4.998766400914329e-05,
      "loss": 1.4793,
      "step": 100
    },
    {
      "epoch": 9.75609756097561,
      "eval_loss": 1.5796997547149658,
      "eval_runtime": 1.8056,
      "eval_samples_per_second": 16.615,
      "eval_steps_per_second": 1.108,
      "step": 100
    },
    {
      "epoch": 10.24390243902439,
      "grad_norm": 0.5018332004547119,
      "learning_rate": 4.998639968473751e-05,
      "loss": 1.6527,
      "step": 105
    },
    {
      "epoch": 10.731707317073171,
      "grad_norm": 0.547624945640564,
      "learning_rate": 4.998507370887433e-05,
      "loss": 1.4162,
      "step": 110
    },
    {
      "epoch": 11.21951219512195,
      "grad_norm": 0.5477908253669739,
      "learning_rate": 4.998368608482546e-05,
      "loss": 1.6662,
      "step": 115
    },
    {
      "epoch": 11.707317073170731,
      "grad_norm": 0.5404742956161499,
      "learning_rate": 4.998223681601473e-05,
      "loss": 1.3959,
      "step": 120
    },
    {
      "epoch": 12.195121951219512,
      "grad_norm": 0.5586562752723694,
      "learning_rate": 4.9980725906018074e-05,
      "loss": 1.5836,
      "step": 125
    },
    {
      "epoch": 12.682926829268293,
      "grad_norm": 0.6067156195640564,
      "learning_rate": 4.99791533585635e-05,
      "loss": 1.359,
      "step": 130
    },
    {
      "epoch": 13.170731707317072,
      "grad_norm": 0.6146095395088196,
      "learning_rate": 4.997751917753113e-05,
      "loss": 1.5528,
      "step": 135
    },
    {
      "epoch": 13.658536585365853,
      "grad_norm": 0.6961056590080261,
      "learning_rate": 4.9975823366953124e-05,
      "loss": 1.3304,
      "step": 140
    },
    {
      "epoch": 14.146341463414634,
      "grad_norm": 0.7301585078239441,
      "learning_rate": 4.9974065931013734e-05,
      "loss": 1.4971,
      "step": 145
    },
    {
      "epoch": 14.634146341463415,
      "grad_norm": 0.7630313038825989,
      "learning_rate": 4.9972246874049254e-05,
      "loss": 1.2871,
      "step": 150
    },
    {
      "epoch": 15.121951219512194,
      "grad_norm": 0.7613312602043152,
      "learning_rate": 4.997036620054803e-05,
      "loss": 1.4357,
      "step": 155
    },
    {
      "epoch": 15.609756097560975,
      "grad_norm": 0.8856669068336487,
      "learning_rate": 4.996842391515044e-05,
      "loss": 1.2189,
      "step": 160
    },
    {
      "epoch": 16.097560975609756,
      "grad_norm": 0.8558287024497986,
      "learning_rate": 4.996642002264887e-05,
      "loss": 1.4158,
      "step": 165
    },
    {
      "epoch": 16.585365853658537,
      "grad_norm": 0.9210456609725952,
      "learning_rate": 4.996435452798774e-05,
      "loss": 1.1756,
      "step": 170
    },
    {
      "epoch": 17.073170731707318,
      "grad_norm": 2.2575154304504395,
      "learning_rate": 4.9962227436263453e-05,
      "loss": 1.334,
      "step": 175
    },
    {
      "epoch": 17.5609756097561,
      "grad_norm": 1.0141328573226929,
      "learning_rate": 4.996003875272438e-05,
      "loss": 1.1209,
      "step": 180
    },
    {
      "epoch": 18.048780487804876,
      "grad_norm": 2.6552112102508545,
      "learning_rate": 4.9957788482770886e-05,
      "loss": 1.2785,
      "step": 185
    },
    {
      "epoch": 18.536585365853657,
      "grad_norm": 1.1513116359710693,
      "learning_rate": 4.99554766319553e-05,
      "loss": 1.077,
      "step": 190
    },
    {
      "epoch": 19.024390243902438,
      "grad_norm": 3.0424864292144775,
      "learning_rate": 4.995310320598187e-05,
      "loss": 1.1781,
      "step": 195
    },
    {
      "epoch": 19.51219512195122,
      "grad_norm": 1.101170301437378,
      "learning_rate": 4.995066821070679e-05,
      "loss": 1.0107,
      "step": 200
    },
    {
      "epoch": 19.51219512195122,
      "eval_loss": 1.7687307596206665,
      "eval_runtime": 1.7997,
      "eval_samples_per_second": 16.669,
      "eval_steps_per_second": 1.111,
      "step": 200
    },
    {
      "epoch": 20.0,
      "grad_norm": 3.1732351779937744,
      "learning_rate": 4.994817165213818e-05,
      "loss": 1.1601,
      "step": 205
    },
    {
      "epoch": 20.48780487804878,
      "grad_norm": 1.257126808166504,
      "learning_rate": 4.994561353643604e-05,
      "loss": 0.9467,
      "step": 210
    },
    {
      "epoch": 20.975609756097562,
      "grad_norm": 1.3430854082107544,
      "learning_rate": 4.994299386991227e-05,
      "loss": 0.9601,
      "step": 215
    },
    {
      "epoch": 21.463414634146343,
      "grad_norm": 1.342464566230774,
      "learning_rate": 4.994031265903063e-05,
      "loss": 1.0379,
      "step": 220
    },
    {
      "epoch": 21.951219512195124,
      "grad_norm": 1.4426069259643555,
      "learning_rate": 4.9937569910406756e-05,
      "loss": 0.8863,
      "step": 225
    },
    {
      "epoch": 22.4390243902439,
      "grad_norm": 1.5005602836608887,
      "learning_rate": 4.9934765630808095e-05,
      "loss": 0.9822,
      "step": 230
    },
    {
      "epoch": 22.926829268292682,
      "grad_norm": 1.5088331699371338,
      "learning_rate": 4.993189982715393e-05,
      "loss": 0.8323,
      "step": 235
    },
    {
      "epoch": 23.414634146341463,
      "grad_norm": 1.5587948560714722,
      "learning_rate": 4.992897250651535e-05,
      "loss": 0.9425,
      "step": 240
    },
    {
      "epoch": 23.902439024390244,
      "grad_norm": 1.632183313369751,
      "learning_rate": 4.992598367611523e-05,
      "loss": 0.795,
      "step": 245
    },
    {
      "epoch": 24.390243902439025,
      "grad_norm": 1.6557881832122803,
      "learning_rate": 4.99229333433282e-05,
      "loss": 0.8744,
      "step": 250
    },
    {
      "epoch": 24.878048780487806,
      "grad_norm": 1.845182180404663,
      "learning_rate": 4.9919821515680665e-05,
      "loss": 0.7552,
      "step": 255
    },
    {
      "epoch": 25.365853658536587,
      "grad_norm": 1.5950100421905518,
      "learning_rate": 4.991664820085074e-05,
      "loss": 0.8265,
      "step": 260
    },
    {
      "epoch": 25.853658536585368,
      "grad_norm": 1.9081616401672363,
      "learning_rate": 4.991341340666829e-05,
      "loss": 0.7154,
      "step": 265
    },
    {
      "epoch": 26.341463414634145,
      "grad_norm": 1.6224268674850464,
      "learning_rate": 4.991011714111481e-05,
      "loss": 0.785,
      "step": 270
    },
    {
      "epoch": 26.829268292682926,
      "grad_norm": 1.9874588251113892,
      "learning_rate": 4.990675941232353e-05,
      "loss": 0.6619,
      "step": 275
    },
    {
      "epoch": 27.317073170731707,
      "grad_norm": 1.815745234489441,
      "learning_rate": 4.990334022857932e-05,
      "loss": 0.7511,
      "step": 280
    },
    {
      "epoch": 27.804878048780488,
      "grad_norm": 2.0271077156066895,
      "learning_rate": 4.989985959831865e-05,
      "loss": 0.6239,
      "step": 285
    },
    {
      "epoch": 28.29268292682927,
      "grad_norm": 1.7398715019226074,
      "learning_rate": 4.989631753012964e-05,
      "loss": 0.715,
      "step": 290
    },
    {
      "epoch": 28.78048780487805,
      "grad_norm": 1.8713070154190063,
      "learning_rate": 4.989271403275201e-05,
      "loss": 0.595,
      "step": 295
    },
    {
      "epoch": 29.26829268292683,
      "grad_norm": 1.7898844480514526,
      "learning_rate": 4.9889049115077005e-05,
      "loss": 0.6672,
      "step": 300
    },
    {
      "epoch": 29.26829268292683,
      "eval_loss": 2.225870132446289,
      "eval_runtime": 1.8177,
      "eval_samples_per_second": 16.504,
      "eval_steps_per_second": 1.1,
      "step": 300
    },
    {
      "epoch": 29.75609756097561,
      "grad_norm": 2.2648677825927734,
      "learning_rate": 4.988532278614746e-05,
      "loss": 0.5647,
      "step": 305
    },
    {
      "epoch": 30.24390243902439,
      "grad_norm": 1.8077315092086792,
      "learning_rate": 4.988153505515771e-05,
      "loss": 0.6036,
      "step": 310
    },
    {
      "epoch": 30.73170731707317,
      "grad_norm": 2.0655148029327393,
      "learning_rate": 4.987768593145362e-05,
      "loss": 0.5235,
      "step": 315
    },
    {
      "epoch": 31.21951219512195,
      "grad_norm": 1.9794917106628418,
      "learning_rate": 4.987377542453251e-05,
      "loss": 0.6037,
      "step": 320
    },
    {
      "epoch": 31.70731707317073,
      "grad_norm": 2.165286064147949,
      "learning_rate": 4.9869803544043166e-05,
      "loss": 0.4936,
      "step": 325
    },
    {
      "epoch": 32.19512195121951,
      "grad_norm": 1.8787338733673096,
      "learning_rate": 4.986577029978581e-05,
      "loss": 0.5382,
      "step": 330
    },
    {
      "epoch": 32.68292682926829,
      "grad_norm": 2.1951825618743896,
      "learning_rate": 4.9861675701712074e-05,
      "loss": 0.4591,
      "step": 335
    },
    {
      "epoch": 33.170731707317074,
      "grad_norm": 2.1286416053771973,
      "learning_rate": 4.9857519759924974e-05,
      "loss": 0.5294,
      "step": 340
    },
    {
      "epoch": 33.65853658536585,
      "grad_norm": 2.3105342388153076,
      "learning_rate": 4.985330248467888e-05,
      "loss": 0.421,
      "step": 345
    },
    {
      "epoch": 34.146341463414636,
      "grad_norm": 2.0151095390319824,
      "learning_rate": 4.98490238863795e-05,
      "loss": 0.4858,
      "step": 350
    },
    {
      "epoch": 34.63414634146341,
      "grad_norm": 2.335913896560669,
      "learning_rate": 4.984468397558384e-05,
      "loss": 0.3916,
      "step": 355
    },
    {
      "epoch": 35.1219512195122,
      "grad_norm": 2.0078792572021484,
      "learning_rate": 4.984028276300021e-05,
      "loss": 0.4611,
      "step": 360
    },
    {
      "epoch": 35.609756097560975,
      "grad_norm": 2.2267966270446777,
      "learning_rate": 4.983582025948817e-05,
      "loss": 0.3797,
      "step": 365
    },
    {
      "epoch": 36.09756097560975,
      "grad_norm": 2.0965986251831055,
      "learning_rate": 4.9831296476058484e-05,
      "loss": 0.427,
      "step": 370
    },
    {
      "epoch": 36.58536585365854,
      "grad_norm": 2.012357473373413,
      "learning_rate": 4.982671142387316e-05,
      "loss": 0.3427,
      "step": 375
    },
    {
      "epoch": 37.073170731707314,
      "grad_norm": 5.198197364807129,
      "learning_rate": 4.982206511424534e-05,
      "loss": 0.3998,
      "step": 380
    },
    {
      "epoch": 37.5609756097561,
      "grad_norm": 1.9303287267684937,
      "learning_rate": 4.981735755863934e-05,
      "loss": 0.3209,
      "step": 385
    },
    {
      "epoch": 38.048780487804876,
      "grad_norm": 4.76200008392334,
      "learning_rate": 4.98125887686706e-05,
      "loss": 0.3744,
      "step": 390
    },
    {
      "epoch": 38.53658536585366,
      "grad_norm": 2.1141130924224854,
      "learning_rate": 4.9807758756105607e-05,
      "loss": 0.3075,
      "step": 395
    },
    {
      "epoch": 39.02439024390244,
      "grad_norm": 5.503283977508545,
      "learning_rate": 4.980286753286195e-05,
      "loss": 0.3399,
      "step": 400
    },
    {
      "epoch": 39.02439024390244,
      "eval_loss": 2.8211026191711426,
      "eval_runtime": 1.8051,
      "eval_samples_per_second": 16.619,
      "eval_steps_per_second": 1.108,
      "step": 400
    },
    {
      "epoch": 39.51219512195122,
      "grad_norm": 2.073908805847168,
      "learning_rate": 4.9797915111008236e-05,
      "loss": 0.2772,
      "step": 405
    },
    {
      "epoch": 40.0,
      "grad_norm": 5.677309513092041,
      "learning_rate": 4.9792901502764075e-05,
      "loss": 0.326,
      "step": 410
    },
    {
      "epoch": 40.48780487804878,
      "grad_norm": 2.1646676063537598,
      "learning_rate": 4.9787826720500044e-05,
      "loss": 0.2674,
      "step": 415
    },
    {
      "epoch": 40.97560975609756,
      "grad_norm": 2.1243717670440674,
      "learning_rate": 4.978269077673767e-05,
      "loss": 0.2584,
      "step": 420
    },
    {
      "epoch": 41.46341463414634,
      "grad_norm": 2.038255453109741,
      "learning_rate": 4.9777493684149375e-05,
      "loss": 0.2898,
      "step": 425
    },
    {
      "epoch": 41.951219512195124,
      "grad_norm": 2.10526967048645,
      "learning_rate": 4.977223545555847e-05,
      "loss": 0.2398,
      "step": 430
    },
    {
      "epoch": 42.4390243902439,
      "grad_norm": 1.8453829288482666,
      "learning_rate": 4.976691610393912e-05,
      "loss": 0.2671,
      "step": 435
    },
    {
      "epoch": 42.926829268292686,
      "grad_norm": 1.8792107105255127,
      "learning_rate": 4.976153564241628e-05,
      "loss": 0.226,
      "step": 440
    },
    {
      "epoch": 43.41463414634146,
      "grad_norm": 1.5820512771606445,
      "learning_rate": 4.9756094084265724e-05,
      "loss": 0.249,
      "step": 445
    },
    {
      "epoch": 43.90243902439025,
      "grad_norm": 1.882997751235962,
      "learning_rate": 4.975059144291394e-05,
      "loss": 0.2153,
      "step": 450
    },
    {
      "epoch": 44.390243902439025,
      "grad_norm": 1.8014826774597168,
      "learning_rate": 4.974502773193816e-05,
      "loss": 0.2305,
      "step": 455
    },
    {
      "epoch": 44.8780487804878,
      "grad_norm": 1.9617403745651245,
      "learning_rate": 4.9739402965066276e-05,
      "loss": 0.1966,
      "step": 460
    },
    {
      "epoch": 45.36585365853659,
      "grad_norm": 1.7406798601150513,
      "learning_rate": 4.973371715617685e-05,
      "loss": 0.2192,
      "step": 465
    },
    {
      "epoch": 45.853658536585364,
      "grad_norm": 2.1264808177948,
      "learning_rate": 4.9727970319299044e-05,
      "loss": 0.1823,
      "step": 470
    },
    {
      "epoch": 46.34146341463415,
      "grad_norm": 2.0575692653656006,
      "learning_rate": 4.972216246861262e-05,
      "loss": 0.2006,
      "step": 475
    },
    {
      "epoch": 46.829268292682926,
      "grad_norm": 2.1835896968841553,
      "learning_rate": 4.971629361844785e-05,
      "loss": 0.1711,
      "step": 480
    },
    {
      "epoch": 47.31707317073171,
      "grad_norm": 1.827802300453186,
      "learning_rate": 4.971036378328556e-05,
      "loss": 0.187,
      "step": 485
    },
    {
      "epoch": 47.80487804878049,
      "grad_norm": 1.9736413955688477,
      "learning_rate": 4.970437297775702e-05,
      "loss": 0.1594,
      "step": 490
    },
    {
      "epoch": 48.292682926829265,
      "grad_norm": 1.8045755624771118,
      "learning_rate": 4.969832121664394e-05,
      "loss": 0.1804,
      "step": 495
    },
    {
      "epoch": 48.78048780487805,
      "grad_norm": 1.8086020946502686,
      "learning_rate": 4.9692208514878444e-05,
      "loss": 0.1507,
      "step": 500
    },
    {
      "epoch": 48.78048780487805,
      "eval_loss": 3.3465559482574463,
      "eval_runtime": 1.8176,
      "eval_samples_per_second": 16.505,
      "eval_steps_per_second": 1.1,
      "step": 500
    },
    {
      "epoch": 49.26829268292683,
      "grad_norm": 1.766282320022583,
      "learning_rate": 4.968603488754302e-05,
      "loss": 0.1649,
      "step": 505
    },
    {
      "epoch": 49.75609756097561,
      "grad_norm": 1.7173364162445068,
      "learning_rate": 4.967980034987048e-05,
      "loss": 0.1402,
      "step": 510
    },
    {
      "epoch": 50.24390243902439,
      "grad_norm": 1.7936052083969116,
      "learning_rate": 4.9673504917243924e-05,
      "loss": 0.1534,
      "step": 515
    },
    {
      "epoch": 50.73170731707317,
      "grad_norm": 1.7856470346450806,
      "learning_rate": 4.96671486051967e-05,
      "loss": 0.1294,
      "step": 520
    },
    {
      "epoch": 51.21951219512195,
      "grad_norm": 1.9331763982772827,
      "learning_rate": 4.966073142941239e-05,
      "loss": 0.155,
      "step": 525
    },
    {
      "epoch": 51.707317073170735,
      "grad_norm": 1.608651041984558,
      "learning_rate": 4.9654253405724724e-05,
      "loss": 0.122,
      "step": 530
    },
    {
      "epoch": 52.19512195121951,
      "grad_norm": 1.6378811597824097,
      "learning_rate": 4.964771455011758e-05,
      "loss": 0.1394,
      "step": 535
    },
    {
      "epoch": 52.68292682926829,
      "grad_norm": 1.9318734407424927,
      "learning_rate": 4.9641114878724956e-05,
      "loss": 0.1135,
      "step": 540
    },
    {
      "epoch": 53.170731707317074,
      "grad_norm": 1.6009440422058105,
      "learning_rate": 4.9634454407830855e-05,
      "loss": 0.1331,
      "step": 545
    },
    {
      "epoch": 53.65853658536585,
      "grad_norm": 1.6944833993911743,
      "learning_rate": 4.962773315386935e-05,
      "loss": 0.1077,
      "step": 550
    },
    {
      "epoch": 54.146341463414636,
      "grad_norm": 2.13407301902771,
      "learning_rate": 4.962095113342445e-05,
      "loss": 0.1225,
      "step": 555
    },
    {
      "epoch": 54.63414634146341,
      "grad_norm": 2.2130420207977295,
      "learning_rate": 4.9614108363230135e-05,
      "loss": 0.1029,
      "step": 560
    },
    {
      "epoch": 55.1219512195122,
      "grad_norm": 1.6568336486816406,
      "learning_rate": 4.9607204860170245e-05,
      "loss": 0.1165,
      "step": 565
    },
    {
      "epoch": 55.609756097560975,
      "grad_norm": 1.665381669998169,
      "learning_rate": 4.9600240641278496e-05,
      "loss": 0.0952,
      "step": 570
    },
    {
      "epoch": 56.09756097560975,
      "grad_norm": 2.0931851863861084,
      "learning_rate": 4.9593215723738404e-05,
      "loss": 0.1125,
      "step": 575
    },
    {
      "epoch": 56.58536585365854,
      "grad_norm": 1.7382311820983887,
      "learning_rate": 4.958613012488324e-05,
      "loss": 0.0927,
      "step": 580
    },
    {
      "epoch": 57.073170731707314,
      "grad_norm": 5.504836082458496,
      "learning_rate": 4.957898386219604e-05,
      "loss": 0.1044,
      "step": 585
    },
    {
      "epoch": 57.5609756097561,
      "grad_norm": 1.901318907737732,
      "learning_rate": 4.957177695330948e-05,
      "loss": 0.0872,
      "step": 590
    },
    {
      "epoch": 58.048780487804876,
      "grad_norm": 4.0037617683410645,
      "learning_rate": 4.95645094160059e-05,
      "loss": 0.104,
      "step": 595
    },
    {
      "epoch": 58.53658536585366,
      "grad_norm": 1.3481358289718628,
      "learning_rate": 4.9557181268217227e-05,
      "loss": 0.0816,
      "step": 600
    },
    {
      "epoch": 58.53658536585366,
      "eval_loss": 3.687638282775879,
      "eval_runtime": 1.8863,
      "eval_samples_per_second": 15.904,
      "eval_steps_per_second": 1.06,
      "step": 600
    },
    {
      "epoch": 59.02439024390244,
      "grad_norm": 4.157949447631836,
      "learning_rate": 4.954979252802492e-05,
      "loss": 0.0978,
      "step": 605
    },
    {
      "epoch": 59.51219512195122,
      "grad_norm": 1.4165152311325073,
      "learning_rate": 4.9542343213659974e-05,
      "loss": 0.0777,
      "step": 610
    },
    {
      "epoch": 60.0,
      "grad_norm": 4.92212438583374,
      "learning_rate": 4.953483334350284e-05,
      "loss": 0.0907,
      "step": 615
    },
    {
      "epoch": 60.48780487804878,
      "grad_norm": 1.2947123050689697,
      "learning_rate": 4.952726293608335e-05,
      "loss": 0.074,
      "step": 620
    },
    {
      "epoch": 60.97560975609756,
      "grad_norm": 2.1517913341522217,
      "learning_rate": 4.951963201008076e-05,
      "loss": 0.0754,
      "step": 625
    },
    {
      "epoch": 61.46341463414634,
      "grad_norm": 1.5529727935791016,
      "learning_rate": 4.951194058432361e-05,
      "loss": 0.0847,
      "step": 630
    },
    {
      "epoch": 61.951219512195124,
      "grad_norm": 1.7139527797698975,
      "learning_rate": 4.9504188677789733e-05,
      "loss": 0.0702,
      "step": 635
    },
    {
      "epoch": 62.4390243902439,
      "grad_norm": 1.28721022605896,
      "learning_rate": 4.949637630960617e-05,
      "loss": 0.0791,
      "step": 640
    },
    {
      "epoch": 62.926829268292686,
      "grad_norm": 1.5656238794326782,
      "learning_rate": 4.948850349904919e-05,
      "loss": 0.0676,
      "step": 645
    },
    {
      "epoch": 63.41463414634146,
      "grad_norm": 1.2967047691345215,
      "learning_rate": 4.9480570265544144e-05,
      "loss": 0.0729,
      "step": 650
    },
    {
      "epoch": 63.90243902439025,
      "grad_norm": 1.350338101387024,
      "learning_rate": 4.9472576628665515e-05,
      "loss": 0.0639,
      "step": 655
    },
    {
      "epoch": 64.39024390243902,
      "grad_norm": 1.6149462461471558,
      "learning_rate": 4.9464522608136805e-05,
      "loss": 0.0727,
      "step": 660
    },
    {
      "epoch": 64.8780487804878,
      "grad_norm": 1.439818263053894,
      "learning_rate": 4.94564082238305e-05,
      "loss": 0.0598,
      "step": 665
    },
    {
      "epoch": 65.36585365853658,
      "grad_norm": 1.6474637985229492,
      "learning_rate": 4.944823349576805e-05,
      "loss": 0.0704,
      "step": 670
    },
    {
      "epoch": 65.85365853658537,
      "grad_norm": 1.244030475616455,
      "learning_rate": 4.943999844411977e-05,
      "loss": 0.0569,
      "step": 675
    },
    {
      "epoch": 66.34146341463415,
      "grad_norm": 1.899165153503418,
      "learning_rate": 4.943170308920484e-05,
      "loss": 0.0619,
      "step": 680
    },
    {
      "epoch": 66.82926829268293,
      "grad_norm": 1.5611094236373901,
      "learning_rate": 4.942334745149122e-05,
      "loss": 0.0554,
      "step": 685
    },
    {
      "epoch": 67.3170731707317,
      "grad_norm": 1.3392466306686401,
      "learning_rate": 4.941493155159562e-05,
      "loss": 0.0609,
      "step": 690
    },
    {
      "epoch": 67.8048780487805,
      "grad_norm": 1.563139796257019,
      "learning_rate": 4.9406455410283426e-05,
      "loss": 0.0533,
      "step": 695
    },
    {
      "epoch": 68.29268292682927,
      "grad_norm": 1.6458849906921387,
      "learning_rate": 4.939791904846869e-05,
      "loss": 0.0593,
      "step": 700
    },
    {
      "epoch": 68.29268292682927,
      "eval_loss": 3.8383266925811768,
      "eval_runtime": 1.8184,
      "eval_samples_per_second": 16.498,
      "eval_steps_per_second": 1.1,
      "step": 700
    },
    {
      "epoch": 68.78048780487805,
      "grad_norm": 1.9120391607284546,
      "learning_rate": 4.938932248721402e-05,
      "loss": 0.0516,
      "step": 705
    },
    {
      "epoch": 69.26829268292683,
      "grad_norm": 1.445043921470642,
      "learning_rate": 4.938066574773059e-05,
      "loss": 0.0583,
      "step": 710
    },
    {
      "epoch": 69.7560975609756,
      "grad_norm": 1.55235755443573,
      "learning_rate": 4.9371948851378035e-05,
      "loss": 0.0489,
      "step": 715
    },
    {
      "epoch": 70.2439024390244,
      "grad_norm": 1.4705082178115845,
      "learning_rate": 4.9363171819664434e-05,
      "loss": 0.0582,
      "step": 720
    },
    {
      "epoch": 70.73170731707317,
      "grad_norm": 1.2283316850662231,
      "learning_rate": 4.935433467424624e-05,
      "loss": 0.0487,
      "step": 725
    },
    {
      "epoch": 71.21951219512195,
      "grad_norm": 1.5826307535171509,
      "learning_rate": 4.934543743692822e-05,
      "loss": 0.0557,
      "step": 730
    },
    {
      "epoch": 71.70731707317073,
      "grad_norm": 1.297240138053894,
      "learning_rate": 4.933648012966344e-05,
      "loss": 0.0467,
      "step": 735
    },
    {
      "epoch": 72.1951219512195,
      "grad_norm": 1.3318045139312744,
      "learning_rate": 4.9327462774553166e-05,
      "loss": 0.0546,
      "step": 740
    },
    {
      "epoch": 72.6829268292683,
      "grad_norm": 2.051762580871582,
      "learning_rate": 4.931838539384681e-05,
      "loss": 0.0455,
      "step": 745
    },
    {
      "epoch": 73.17073170731707,
      "grad_norm": 1.5244779586791992,
      "learning_rate": 4.9309248009941914e-05,
      "loss": 0.0523,
      "step": 750
    },
    {
      "epoch": 73.65853658536585,
      "grad_norm": 1.0968031883239746,
      "learning_rate": 4.9300050645384065e-05,
      "loss": 0.0435,
      "step": 755
    },
    {
      "epoch": 74.14634146341463,
      "grad_norm": 0.8623814582824707,
      "learning_rate": 4.929079332286685e-05,
      "loss": 0.0494,
      "step": 760
    },
    {
      "epoch": 74.63414634146342,
      "grad_norm": 1.0581809282302856,
      "learning_rate": 4.928147606523178e-05,
      "loss": 0.0407,
      "step": 765
    },
    {
      "epoch": 75.1219512195122,
      "grad_norm": 1.1478919982910156,
      "learning_rate": 4.9272098895468277e-05,
      "loss": 0.0484,
      "step": 770
    },
    {
      "epoch": 75.60975609756098,
      "grad_norm": 0.9831007719039917,
      "learning_rate": 4.9262661836713564e-05,
      "loss": 0.0391,
      "step": 775
    },
    {
      "epoch": 76.09756097560975,
      "grad_norm": 0.9867630004882812,
      "learning_rate": 4.925316491225265e-05,
      "loss": 0.0447,
      "step": 780
    },
    {
      "epoch": 76.58536585365853,
      "grad_norm": 1.6200284957885742,
      "learning_rate": 4.924360814551825e-05,
      "loss": 0.037,
      "step": 785
    },
    {
      "epoch": 77.07317073170732,
      "grad_norm": 2.4452478885650635,
      "learning_rate": 4.923399156009073e-05,
      "loss": 0.0431,
      "step": 790
    },
    {
      "epoch": 77.5609756097561,
      "grad_norm": 1.2785996198654175,
      "learning_rate": 4.922431517969808e-05,
      "loss": 0.037,
      "step": 795
    },
    {
      "epoch": 78.04878048780488,
      "grad_norm": 2.8262429237365723,
      "learning_rate": 4.9214579028215776e-05,
      "loss": 0.0427,
      "step": 800
    },
    {
      "epoch": 78.04878048780488,
      "eval_loss": 4.007619380950928,
      "eval_runtime": 1.8051,
      "eval_samples_per_second": 16.619,
      "eval_steps_per_second": 1.108,
      "step": 800
    },
    {
      "epoch": 78.53658536585365,
      "grad_norm": 1.2974997758865356,
      "learning_rate": 4.920478312966683e-05,
      "loss": 0.0353,
      "step": 805
    },
    {
      "epoch": 79.02439024390245,
      "grad_norm": 6.701072692871094,
      "learning_rate": 4.919492750822164e-05,
      "loss": 0.0425,
      "step": 810
    },
    {
      "epoch": 79.51219512195122,
      "grad_norm": 1.7485984563827515,
      "learning_rate": 4.9185012188197965e-05,
      "loss": 0.0357,
      "step": 815
    },
    {
      "epoch": 80.0,
      "grad_norm": 3.158027172088623,
      "learning_rate": 4.917503719406088e-05,
      "loss": 0.0416,
      "step": 820
    },
    {
      "epoch": 80.48780487804878,
      "grad_norm": 1.3409194946289062,
      "learning_rate": 4.916500255042268e-05,
      "loss": 0.0326,
      "step": 825
    },
    {
      "epoch": 80.97560975609755,
      "grad_norm": 1.895443081855774,
      "learning_rate": 4.915490828204287e-05,
      "loss": 0.0355,
      "step": 830
    },
    {
      "epoch": 81.46341463414635,
      "grad_norm": 1.1661787033081055,
      "learning_rate": 4.914475441382804e-05,
      "loss": 0.0395,
      "step": 835
    },
    {
      "epoch": 81.95121951219512,
      "grad_norm": 1.1906940937042236,
      "learning_rate": 4.913454097083185e-05,
      "loss": 0.0343,
      "step": 840
    },
    {
      "epoch": 82.4390243902439,
      "grad_norm": 1.5033636093139648,
      "learning_rate": 4.9124267978254956e-05,
      "loss": 0.0379,
      "step": 845
    },
    {
      "epoch": 82.92682926829268,
      "grad_norm": 1.6718833446502686,
      "learning_rate": 4.9113935461444955e-05,
      "loss": 0.0345,
      "step": 850
    },
    {
      "epoch": 83.41463414634147,
      "grad_norm": 1.5766974687576294,
      "learning_rate": 4.9103543445896296e-05,
      "loss": 0.037,
      "step": 855
    },
    {
      "epoch": 83.90243902439025,
      "grad_norm": 1.4443912506103516,
      "learning_rate": 4.909309195725025e-05,
      "loss": 0.0327,
      "step": 860
    },
    {
      "epoch": 84.39024390243902,
      "grad_norm": 1.4687420129776,
      "learning_rate": 4.908258102129481e-05,
      "loss": 0.0362,
      "step": 865
    },
    {
      "epoch": 84.8780487804878,
      "grad_norm": 1.528295636177063,
      "learning_rate": 4.907201066396469e-05,
      "loss": 0.0324,
      "step": 870
    },
    {
      "epoch": 85.36585365853658,
      "grad_norm": 1.0552427768707275,
      "learning_rate": 4.906138091134118e-05,
      "loss": 0.0358,
      "step": 875
    },
    {
      "epoch": 85.85365853658537,
      "grad_norm": 1.011768102645874,
      "learning_rate": 4.905069178965215e-05,
      "loss": 0.0309,
      "step": 880
    },
    {
      "epoch": 86.34146341463415,
      "grad_norm": 1.0609345436096191,
      "learning_rate": 4.903994332527193e-05,
      "loss": 0.0347,
      "step": 885
    },
    {
      "epoch": 86.82926829268293,
      "grad_norm": 1.118039846420288,
      "learning_rate": 4.90291355447213e-05,
      "loss": 0.0299,
      "step": 890
    },
    {
      "epoch": 87.3170731707317,
      "grad_norm": 1.2681002616882324,
      "learning_rate": 4.901826847466738e-05,
      "loss": 0.0336,
      "step": 895
    },
    {
      "epoch": 87.8048780487805,
      "grad_norm": 1.218329668045044,
      "learning_rate": 4.900734214192358e-05,
      "loss": 0.0286,
      "step": 900
    },
    {
      "epoch": 87.8048780487805,
      "eval_loss": 4.090958595275879,
      "eval_runtime": 1.8198,
      "eval_samples_per_second": 16.485,
      "eval_steps_per_second": 1.099,
      "step": 900
    },
    {
      "epoch": 88.29268292682927,
      "grad_norm": 0.8934339880943298,
      "learning_rate": 4.899635657344954e-05,
      "loss": 0.0323,
      "step": 905
    },
    {
      "epoch": 88.78048780487805,
      "grad_norm": 1.214098334312439,
      "learning_rate": 4.898531179635107e-05,
      "loss": 0.0289,
      "step": 910
    },
    {
      "epoch": 89.26829268292683,
      "grad_norm": 0.9134090542793274,
      "learning_rate": 4.8974207837880053e-05,
      "loss": 0.03,
      "step": 915
    },
    {
      "epoch": 89.7560975609756,
      "grad_norm": 1.2110153436660767,
      "learning_rate": 4.89630447254344e-05,
      "loss": 0.0279,
      "step": 920
    },
    {
      "epoch": 90.2439024390244,
      "grad_norm": 1.306756615638733,
      "learning_rate": 4.8951822486557986e-05,
      "loss": 0.0313,
      "step": 925
    },
    {
      "epoch": 90.73170731707317,
      "grad_norm": 1.25594961643219,
      "learning_rate": 4.8940541148940555e-05,
      "loss": 0.0262,
      "step": 930
    },
    {
      "epoch": 91.21951219512195,
      "grad_norm": 1.116240382194519,
      "learning_rate": 4.8929200740417716e-05,
      "loss": 0.0304,
      "step": 935
    },
    {
      "epoch": 91.70731707317073,
      "grad_norm": 1.353734016418457,
      "learning_rate": 4.891780128897077e-05,
      "loss": 0.0258,
      "step": 940
    },
    {
      "epoch": 92.1951219512195,
      "grad_norm": 0.8611592054367065,
      "learning_rate": 4.890634282272674e-05,
      "loss": 0.0297,
      "step": 945
    },
    {
      "epoch": 92.6829268292683,
      "grad_norm": 0.6590719819068909,
      "learning_rate": 4.8894825369958255e-05,
      "loss": 0.0238,
      "step": 950
    },
    {
      "epoch": 93.17073170731707,
      "grad_norm": 1.0808788537979126,
      "learning_rate": 4.888324895908349e-05,
      "loss": 0.0282,
      "step": 955
    },
    {
      "epoch": 93.65853658536585,
      "grad_norm": 0.8392329216003418,
      "learning_rate": 4.887161361866608e-05,
      "loss": 0.0229,
      "step": 960
    },
    {
      "epoch": 94.14634146341463,
      "grad_norm": 0.9433706402778625,
      "learning_rate": 4.885991937741507e-05,
      "loss": 0.0289,
      "step": 965
    },
    {
      "epoch": 94.63414634146342,
      "grad_norm": 1.2716906070709229,
      "learning_rate": 4.8848166264184844e-05,
      "loss": 0.0245,
      "step": 970
    },
    {
      "epoch": 95.1219512195122,
      "grad_norm": 0.832461953163147,
      "learning_rate": 4.8836354307975026e-05,
      "loss": 0.0273,
      "step": 975
    },
    {
      "epoch": 95.60975609756098,
      "grad_norm": 1.0942374467849731,
      "learning_rate": 4.882448353793048e-05,
      "loss": 0.0225,
      "step": 980
    },
    {
      "epoch": 96.09756097560975,
      "grad_norm": 0.9455503821372986,
      "learning_rate": 4.88125539833411e-05,
      "loss": 0.0271,
      "step": 985
    },
    {
      "epoch": 96.58536585365853,
      "grad_norm": 1.654769778251648,
      "learning_rate": 4.880056567364192e-05,
      "loss": 0.0233,
      "step": 990
    },
    {
      "epoch": 97.07317073170732,
      "grad_norm": 2.420041561126709,
      "learning_rate": 4.878851863841287e-05,
      "loss": 0.0263,
      "step": 995
    },
    {
      "epoch": 97.5609756097561,
      "grad_norm": 1.0218560695648193,
      "learning_rate": 4.877641290737884e-05,
      "loss": 0.0225,
      "step": 1000
    },
    {
      "epoch": 97.5609756097561,
      "eval_loss": 4.151012897491455,
      "eval_runtime": 1.8193,
      "eval_samples_per_second": 16.49,
      "eval_steps_per_second": 1.099,
      "step": 1000
    },
    {
      "epoch": 98.04878048780488,
      "grad_norm": 1.2887238264083862,
      "learning_rate": 4.8764248510409505e-05,
      "loss": 0.0279,
      "step": 1005
    },
    {
      "epoch": 98.53658536585365,
      "grad_norm": 0.7849061489105225,
      "learning_rate": 4.8752025477519295e-05,
      "loss": 0.0224,
      "step": 1010
    },
    {
      "epoch": 99.02439024390245,
      "grad_norm": 1.901503086090088,
      "learning_rate": 4.873974383886735e-05,
      "loss": 0.0267,
      "step": 1015
    },
    {
      "epoch": 99.51219512195122,
      "grad_norm": 1.0042613744735718,
      "learning_rate": 4.8727403624757365e-05,
      "loss": 0.022,
      "step": 1020
    },
    {
      "epoch": 100.0,
      "grad_norm": 6.382297515869141,
      "learning_rate": 4.8715004865637614e-05,
      "loss": 0.0269,
      "step": 1025
    },
    {
      "epoch": 100.48780487804878,
      "grad_norm": 1.0272077322006226,
      "learning_rate": 4.87025475921008e-05,
      "loss": 0.0219,
      "step": 1030
    },
    {
      "epoch": 100.97560975609755,
      "grad_norm": 1.1292054653167725,
      "learning_rate": 4.8690031834884e-05,
      "loss": 0.0221,
      "step": 1035
    },
    {
      "epoch": 101.46341463414635,
      "grad_norm": 1.310622215270996,
      "learning_rate": 4.867745762486861e-05,
      "loss": 0.0255,
      "step": 1040
    },
    {
      "epoch": 101.95121951219512,
      "grad_norm": 1.4172937870025635,
      "learning_rate": 4.866482499308023e-05,
      "loss": 0.0217,
      "step": 1045
    },
    {
      "epoch": 102.4390243902439,
      "grad_norm": 0.8348469138145447,
      "learning_rate": 4.8652133970688636e-05,
      "loss": 0.0236,
      "step": 1050
    },
    {
      "epoch": 102.92682926829268,
      "grad_norm": 1.052715539932251,
      "learning_rate": 4.863938458900765e-05,
      "loss": 0.0213,
      "step": 1055
    },
    {
      "epoch": 103.41463414634147,
      "grad_norm": 1.1097400188446045,
      "learning_rate": 4.862657687949512e-05,
      "loss": 0.0238,
      "step": 1060
    },
    {
      "epoch": 103.90243902439025,
      "grad_norm": 1.5334725379943848,
      "learning_rate": 4.861371087375279e-05,
      "loss": 0.0197,
      "step": 1065
    },
    {
      "epoch": 104.39024390243902,
      "grad_norm": 1.2322887182235718,
      "learning_rate": 4.860078660352625e-05,
      "loss": 0.0234,
      "step": 1070
    },
    {
      "epoch": 104.8780487804878,
      "grad_norm": 0.9222164750099182,
      "learning_rate": 4.8587804100704845e-05,
      "loss": 0.0206,
      "step": 1075
    },
    {
      "epoch": 105.36585365853658,
      "grad_norm": 0.963853657245636,
      "learning_rate": 4.8574763397321614e-05,
      "loss": 0.0226,
      "step": 1080
    },
    {
      "epoch": 105.85365853658537,
      "grad_norm": 0.5402308702468872,
      "learning_rate": 4.85616645255532e-05,
      "loss": 0.0204,
      "step": 1085
    },
    {
      "epoch": 106.34146341463415,
      "grad_norm": 0.7248349785804749,
      "learning_rate": 4.854850751771977e-05,
      "loss": 0.022,
      "step": 1090
    },
    {
      "epoch": 106.82926829268293,
      "grad_norm": 1.1285464763641357,
      "learning_rate": 4.8535292406284924e-05,
      "loss": 0.0192,
      "step": 1095
    },
    {
      "epoch": 107.3170731707317,
      "grad_norm": 0.735421359539032,
      "learning_rate": 4.852201922385564e-05,
      "loss": 0.0209,
      "step": 1100
    },
    {
      "epoch": 107.3170731707317,
      "eval_loss": 4.279423713684082,
      "eval_runtime": 1.8197,
      "eval_samples_per_second": 16.486,
      "eval_steps_per_second": 1.099,
      "step": 1100
    },
    {
      "epoch": 107.8048780487805,
      "grad_norm": 0.6893799901008606,
      "learning_rate": 4.850868800318218e-05,
      "loss": 0.0178,
      "step": 1105
    },
    {
      "epoch": 108.29268292682927,
      "grad_norm": 0.610765278339386,
      "learning_rate": 4.849529877715799e-05,
      "loss": 0.0208,
      "step": 1110
    },
    {
      "epoch": 108.78048780487805,
      "grad_norm": 0.4853384792804718,
      "learning_rate": 4.848185157881968e-05,
      "loss": 0.017,
      "step": 1115
    },
    {
      "epoch": 109.26829268292683,
      "grad_norm": 1.1619908809661865,
      "learning_rate": 4.846834644134686e-05,
      "loss": 0.0189,
      "step": 1120
    },
    {
      "epoch": 109.7560975609756,
      "grad_norm": 0.7730602622032166,
      "learning_rate": 4.8454783398062106e-05,
      "loss": 0.0161,
      "step": 1125
    },
    {
      "epoch": 110.2439024390244,
      "grad_norm": 0.7500603199005127,
      "learning_rate": 4.844116248243089e-05,
      "loss": 0.0183,
      "step": 1130
    },
    {
      "epoch": 110.73170731707317,
      "grad_norm": 0.7389844059944153,
      "learning_rate": 4.8427483728061475e-05,
      "loss": 0.0163,
      "step": 1135
    },
    {
      "epoch": 111.21951219512195,
      "grad_norm": 0.36880961060523987,
      "learning_rate": 4.841374716870481e-05,
      "loss": 0.018,
      "step": 1140
    },
    {
      "epoch": 111.70731707317073,
      "grad_norm": 0.9224466681480408,
      "learning_rate": 4.83999528382545e-05,
      "loss": 0.016,
      "step": 1145
    },
    {
      "epoch": 112.1951219512195,
      "grad_norm": 0.7632866501808167,
      "learning_rate": 4.838610077074669e-05,
      "loss": 0.0177,
      "step": 1150
    },
    {
      "epoch": 112.6829268292683,
      "grad_norm": 0.7919957041740417,
      "learning_rate": 4.8372191000359955e-05,
      "loss": 0.015,
      "step": 1155
    },
    {
      "epoch": 113.17073170731707,
      "grad_norm": 0.45128703117370605,
      "learning_rate": 4.8358223561415304e-05,
      "loss": 0.0177,
      "step": 1160
    },
    {
      "epoch": 113.65853658536585,
      "grad_norm": 0.520538330078125,
      "learning_rate": 4.834419848837598e-05,
      "loss": 0.0144,
      "step": 1165
    },
    {
      "epoch": 114.14634146341463,
      "grad_norm": 0.9722787141799927,
      "learning_rate": 4.8330115815847465e-05,
      "loss": 0.0167,
      "step": 1170
    },
    {
      "epoch": 114.63414634146342,
      "grad_norm": 0.7013752460479736,
      "learning_rate": 4.8315975578577355e-05,
      "loss": 0.0144,
      "step": 1175
    },
    {
      "epoch": 115.1219512195122,
      "grad_norm": 0.3697745203971863,
      "learning_rate": 4.8301777811455276e-05,
      "loss": 0.016,
      "step": 1180
    },
    {
      "epoch": 115.60975609756098,
      "grad_norm": 0.6704944372177124,
      "learning_rate": 4.828752254951281e-05,
      "loss": 0.0134,
      "step": 1185
    },
    {
      "epoch": 116.09756097560975,
      "grad_norm": 0.2362198680639267,
      "learning_rate": 4.827320982792339e-05,
      "loss": 0.0161,
      "step": 1190
    },
    {
      "epoch": 116.58536585365853,
      "grad_norm": 0.49424663186073303,
      "learning_rate": 4.825883968200225e-05,
      "loss": 0.0129,
      "step": 1195
    },
    {
      "epoch": 117.07317073170732,
      "grad_norm": 1.3535176515579224,
      "learning_rate": 4.8244412147206284e-05,
      "loss": 0.0151,
      "step": 1200
    },
    {
      "epoch": 117.07317073170732,
      "eval_loss": 4.364812850952148,
      "eval_runtime": 1.8027,
      "eval_samples_per_second": 16.641,
      "eval_steps_per_second": 1.109,
      "step": 1200
    },
    {
      "epoch": 117.5609756097561,
      "grad_norm": 0.3276126980781555,
      "learning_rate": 4.8229927259134014e-05,
      "loss": 0.0117,
      "step": 1205
    },
    {
      "epoch": 118.04878048780488,
      "grad_norm": 0.45696884393692017,
      "learning_rate": 4.821538505352543e-05,
      "loss": 0.0146,
      "step": 1210
    },
    {
      "epoch": 118.53658536585365,
      "grad_norm": 0.3885471522808075,
      "learning_rate": 4.820078556626202e-05,
      "loss": 0.0122,
      "step": 1215
    },
    {
      "epoch": 119.02439024390245,
      "grad_norm": 1.2143056392669678,
      "learning_rate": 4.818612883336654e-05,
      "loss": 0.0155,
      "step": 1220
    },
    {
      "epoch": 119.51219512195122,
      "grad_norm": 0.6217185258865356,
      "learning_rate": 4.817141489100302e-05,
      "loss": 0.0122,
      "step": 1225
    },
    {
      "epoch": 120.0,
      "grad_norm": 1.4720594882965088,
      "learning_rate": 4.8156643775476664e-05,
      "loss": 0.0156,
      "step": 1230
    },
    {
      "epoch": 120.48780487804878,
      "grad_norm": 0.30168747901916504,
      "learning_rate": 4.8141815523233734e-05,
      "loss": 0.0119,
      "step": 1235
    },
    {
      "epoch": 120.97560975609755,
      "grad_norm": 0.6306167840957642,
      "learning_rate": 4.812693017086145e-05,
      "loss": 0.012,
      "step": 1240
    },
    {
      "epoch": 121.46341463414635,
      "grad_norm": 0.5193819403648376,
      "learning_rate": 4.811198775508796e-05,
      "loss": 0.0132,
      "step": 1245
    },
    {
      "epoch": 121.95121951219512,
      "grad_norm": 0.49658167362213135,
      "learning_rate": 4.8096988312782174e-05,
      "loss": 0.0116,
      "step": 1250
    },
    {
      "epoch": 122.4390243902439,
      "grad_norm": 0.5228168368339539,
      "learning_rate": 4.8081931880953726e-05,
      "loss": 0.0128,
      "step": 1255
    },
    {
      "epoch": 122.92682926829268,
      "grad_norm": 0.588812530040741,
      "learning_rate": 4.8066818496752875e-05,
      "loss": 0.0116,
      "step": 1260
    },
    {
      "epoch": 123.41463414634147,
      "grad_norm": 0.4247477054595947,
      "learning_rate": 4.805164819747038e-05,
      "loss": 0.0129,
      "step": 1265
    },
    {
      "epoch": 123.90243902439025,
      "grad_norm": 0.6303321719169617,
      "learning_rate": 4.803642102053746e-05,
      "loss": 0.0125,
      "step": 1270
    },
    {
      "epoch": 124.39024390243902,
      "grad_norm": 0.3794728219509125,
      "learning_rate": 4.8021137003525664e-05,
      "loss": 0.0124,
      "step": 1275
    },
    {
      "epoch": 124.8780487804878,
      "grad_norm": 0.5866687297821045,
      "learning_rate": 4.800579618414676e-05,
      "loss": 0.0114,
      "step": 1280
    },
    {
      "epoch": 125.36585365853658,
      "grad_norm": 0.29177922010421753,
      "learning_rate": 4.799039860025272e-05,
      "loss": 0.0125,
      "step": 1285
    },
    {
      "epoch": 125.85365853658537,
      "grad_norm": 1.0940500497817993,
      "learning_rate": 4.797494428983553e-05,
      "loss": 0.0108,
      "step": 1290
    },
    {
      "epoch": 126.34146341463415,
      "grad_norm": 1.1390197277069092,
      "learning_rate": 4.795943329102719e-05,
      "loss": 0.0129,
      "step": 1295
    },
    {
      "epoch": 126.82926829268293,
      "grad_norm": 0.7119138240814209,
      "learning_rate": 4.794386564209953e-05,
      "loss": 0.0108,
      "step": 1300
    },
    {
      "epoch": 126.82926829268293,
      "eval_loss": 4.450249195098877,
      "eval_runtime": 1.8078,
      "eval_samples_per_second": 16.595,
      "eval_steps_per_second": 1.106,
      "step": 1300
    },
    {
      "epoch": 127.3170731707317,
      "grad_norm": 0.9593392610549927,
      "learning_rate": 4.792824138146418e-05,
      "loss": 0.0126,
      "step": 1305
    },
    {
      "epoch": 127.8048780487805,
      "grad_norm": 0.8124863505363464,
      "learning_rate": 4.791256054767245e-05,
      "loss": 0.0116,
      "step": 1310
    },
    {
      "epoch": 128.29268292682926,
      "grad_norm": 0.7263824939727783,
      "learning_rate": 4.789682317941524e-05,
      "loss": 0.0133,
      "step": 1315
    },
    {
      "epoch": 128.78048780487805,
      "grad_norm": 0.8609692454338074,
      "learning_rate": 4.788102931552294e-05,
      "loss": 0.0123,
      "step": 1320
    },
    {
      "epoch": 129.26829268292684,
      "grad_norm": 1.0272958278656006,
      "learning_rate": 4.7865178994965344e-05,
      "loss": 0.0138,
      "step": 1325
    },
    {
      "epoch": 129.7560975609756,
      "grad_norm": 1.0579891204833984,
      "learning_rate": 4.784927225685153e-05,
      "loss": 0.0127,
      "step": 1330
    },
    {
      "epoch": 130.2439024390244,
      "grad_norm": 1.6074327230453491,
      "learning_rate": 4.78333091404298e-05,
      "loss": 0.0147,
      "step": 1335
    },
    {
      "epoch": 130.73170731707316,
      "grad_norm": 0.897225022315979,
      "learning_rate": 4.7817289685087577e-05,
      "loss": 0.0136,
      "step": 1340
    },
    {
      "epoch": 131.21951219512195,
      "grad_norm": 1.0808988809585571,
      "learning_rate": 4.780121393035124e-05,
      "loss": 0.0175,
      "step": 1345
    },
    {
      "epoch": 131.70731707317074,
      "grad_norm": 0.8103275895118713,
      "learning_rate": 4.7785081915886134e-05,
      "loss": 0.0143,
      "step": 1350
    },
    {
      "epoch": 132.1951219512195,
      "grad_norm": 1.1243823766708374,
      "learning_rate": 4.7768893681496394e-05,
      "loss": 0.0183,
      "step": 1355
    },
    {
      "epoch": 132.6829268292683,
      "grad_norm": 1.4739677906036377,
      "learning_rate": 4.775264926712489e-05,
      "loss": 0.0147,
      "step": 1360
    },
    {
      "epoch": 133.17073170731706,
      "grad_norm": 0.9961696863174438,
      "learning_rate": 4.773634871285309e-05,
      "loss": 0.0177,
      "step": 1365
    },
    {
      "epoch": 133.65853658536585,
      "grad_norm": 1.5246468782424927,
      "learning_rate": 4.7719992058901006e-05,
      "loss": 0.0141,
      "step": 1370
    },
    {
      "epoch": 134.14634146341464,
      "grad_norm": 0.7379319071769714,
      "learning_rate": 4.7703579345627035e-05,
      "loss": 0.0168,
      "step": 1375
    },
    {
      "epoch": 134.6341463414634,
      "grad_norm": 1.0078179836273193,
      "learning_rate": 4.7687110613527926e-05,
      "loss": 0.0133,
      "step": 1380
    },
    {
      "epoch": 135.1219512195122,
      "grad_norm": 0.7065199613571167,
      "learning_rate": 4.767058590323864e-05,
      "loss": 0.0164,
      "step": 1385
    },
    {
      "epoch": 135.609756097561,
      "grad_norm": 1.025876522064209,
      "learning_rate": 4.7654005255532244e-05,
      "loss": 0.0132,
      "step": 1390
    },
    {
      "epoch": 136.09756097560975,
      "grad_norm": 1.3312304019927979,
      "learning_rate": 4.763736871131986e-05,
      "loss": 0.0157,
      "step": 1395
    },
    {
      "epoch": 136.58536585365854,
      "grad_norm": 0.878646194934845,
      "learning_rate": 4.762067631165049e-05,
      "loss": 0.0127,
      "step": 1400
    },
    {
      "epoch": 136.58536585365854,
      "eval_loss": 4.401661396026611,
      "eval_runtime": 1.8176,
      "eval_samples_per_second": 16.505,
      "eval_steps_per_second": 1.1,
      "step": 1400
    },
    {
      "epoch": 137.0731707317073,
      "grad_norm": 1.8175089359283447,
      "learning_rate": 4.760392809771098e-05,
      "loss": 0.0158,
      "step": 1405
    },
    {
      "epoch": 137.5609756097561,
      "grad_norm": 1.559312105178833,
      "learning_rate": 4.7587124110825875e-05,
      "loss": 0.0133,
      "step": 1410
    },
    {
      "epoch": 138.0487804878049,
      "grad_norm": 3.578091859817505,
      "learning_rate": 4.757026439245735e-05,
      "loss": 0.0161,
      "step": 1415
    },
    {
      "epoch": 138.53658536585365,
      "grad_norm": 0.739342212677002,
      "learning_rate": 4.755334898420507e-05,
      "loss": 0.0132,
      "step": 1420
    },
    {
      "epoch": 139.02439024390245,
      "grad_norm": 4.083043575286865,
      "learning_rate": 4.753637792780614e-05,
      "loss": 0.0158,
      "step": 1425
    },
    {
      "epoch": 139.5121951219512,
      "grad_norm": 1.1498414278030396,
      "learning_rate": 4.751935126513496e-05,
      "loss": 0.0127,
      "step": 1430
    },
    {
      "epoch": 140.0,
      "grad_norm": 1.8610382080078125,
      "learning_rate": 4.75022690382031e-05,
      "loss": 0.0159,
      "step": 1435
    },
    {
      "epoch": 140.4878048780488,
      "grad_norm": 1.0795520544052124,
      "learning_rate": 4.7485131289159276e-05,
      "loss": 0.0127,
      "step": 1440
    },
    {
      "epoch": 140.97560975609755,
      "grad_norm": 1.3945142030715942,
      "learning_rate": 4.746793806028919e-05,
      "loss": 0.0133,
      "step": 1445
    },
    {
      "epoch": 141.46341463414635,
      "grad_norm": 1.3213523626327515,
      "learning_rate": 4.745068939401539e-05,
      "loss": 0.0134,
      "step": 1450
    },
    {
      "epoch": 141.9512195121951,
      "grad_norm": 1.219024896621704,
      "learning_rate": 4.743338533289728e-05,
      "loss": 0.0132,
      "step": 1455
    },
    {
      "epoch": 142.4390243902439,
      "grad_norm": 0.8004932999610901,
      "learning_rate": 4.7416025919630904e-05,
      "loss": 0.0149,
      "step": 1460
    },
    {
      "epoch": 142.9268292682927,
      "grad_norm": 1.2050635814666748,
      "learning_rate": 4.739861119704887e-05,
      "loss": 0.0134,
      "step": 1465
    },
    {
      "epoch": 143.41463414634146,
      "grad_norm": 0.809335470199585,
      "learning_rate": 4.7381141208120296e-05,
      "loss": 0.0137,
      "step": 1470
    },
    {
      "epoch": 143.90243902439025,
      "grad_norm": 1.162691354751587,
      "learning_rate": 4.7363615995950626e-05,
      "loss": 0.0124,
      "step": 1475
    },
    {
      "epoch": 144.390243902439,
      "grad_norm": 0.5672113299369812,
      "learning_rate": 4.73460356037816e-05,
      "loss": 0.0149,
      "step": 1480
    },
    {
      "epoch": 144.8780487804878,
      "grad_norm": 0.9180819392204285,
      "learning_rate": 4.7328400074991065e-05,
      "loss": 0.0121,
      "step": 1485
    },
    {
      "epoch": 145.3658536585366,
      "grad_norm": 0.7943958640098572,
      "learning_rate": 4.731070945309295e-05,
      "loss": 0.0136,
      "step": 1490
    },
    {
      "epoch": 145.85365853658536,
      "grad_norm": 1.1173030138015747,
      "learning_rate": 4.7292963781737097e-05,
      "loss": 0.0123,
      "step": 1495
    },
    {
      "epoch": 146.34146341463415,
      "grad_norm": 0.663303554058075,
      "learning_rate": 4.72751631047092e-05,
      "loss": 0.0139,
      "step": 1500
    },
    {
      "epoch": 146.34146341463415,
      "eval_loss": 4.354604244232178,
      "eval_runtime": 1.819,
      "eval_samples_per_second": 16.493,
      "eval_steps_per_second": 1.1,
      "step": 1500
    },
    {
      "epoch": 146.82926829268294,
      "grad_norm": 1.0902619361877441,
      "learning_rate": 4.7257307465930644e-05,
      "loss": 0.0122,
      "step": 1505
    },
    {
      "epoch": 147.3170731707317,
      "grad_norm": 0.6334295868873596,
      "learning_rate": 4.723939690945846e-05,
      "loss": 0.0136,
      "step": 1510
    },
    {
      "epoch": 147.8048780487805,
      "grad_norm": 1.0226045846939087,
      "learning_rate": 4.722143147948514e-05,
      "loss": 0.0134,
      "step": 1515
    },
    {
      "epoch": 148.29268292682926,
      "grad_norm": 1.3901365995407104,
      "learning_rate": 4.720341122033862e-05,
      "loss": 0.0147,
      "step": 1520
    },
    {
      "epoch": 148.78048780487805,
      "grad_norm": 0.7607001662254333,
      "learning_rate": 4.718533617648209e-05,
      "loss": 0.0123,
      "step": 1525
    },
    {
      "epoch": 149.26829268292684,
      "grad_norm": 0.8179593682289124,
      "learning_rate": 4.716720639251392e-05,
      "loss": 0.0143,
      "step": 1530
    },
    {
      "epoch": 149.7560975609756,
      "grad_norm": 0.6726498007774353,
      "learning_rate": 4.714902191316755e-05,
      "loss": 0.0121,
      "step": 1535
    },
    {
      "epoch": 150.2439024390244,
      "grad_norm": 0.6926912665367126,
      "learning_rate": 4.713078278331138e-05,
      "loss": 0.0136,
      "step": 1540
    },
    {
      "epoch": 150.73170731707316,
      "grad_norm": 1.0481319427490234,
      "learning_rate": 4.7112489047948655e-05,
      "loss": 0.0124,
      "step": 1545
    },
    {
      "epoch": 151.21951219512195,
      "grad_norm": 1.1764293909072876,
      "learning_rate": 4.709414075221734e-05,
      "loss": 0.0136,
      "step": 1550
    },
    {
      "epoch": 151.70731707317074,
      "grad_norm": 0.7965272068977356,
      "learning_rate": 4.707573794139003e-05,
      "loss": 0.0119,
      "step": 1555
    },
    {
      "epoch": 152.1951219512195,
      "grad_norm": 0.6296132802963257,
      "learning_rate": 4.7057280660873835e-05,
      "loss": 0.0123,
      "step": 1560
    },
    {
      "epoch": 152.6829268292683,
      "grad_norm": 0.803331196308136,
      "learning_rate": 4.7038768956210256e-05,
      "loss": 0.0105,
      "step": 1565
    },
    {
      "epoch": 153.17073170731706,
      "grad_norm": 0.6359063982963562,
      "learning_rate": 4.702020287307509e-05,
      "loss": 0.0117,
      "step": 1570
    },
    {
      "epoch": 153.65853658536585,
      "grad_norm": 0.44671404361724854,
      "learning_rate": 4.7001582457278304e-05,
      "loss": 0.0094,
      "step": 1575
    },
    {
      "epoch": 154.14634146341464,
      "grad_norm": 0.5017270445823669,
      "learning_rate": 4.6982907754763906e-05,
      "loss": 0.0109,
      "step": 1580
    },
    {
      "epoch": 154.6341463414634,
      "grad_norm": 0.8893932700157166,
      "learning_rate": 4.696417881160989e-05,
      "loss": 0.009,
      "step": 1585
    },
    {
      "epoch": 155.1219512195122,
      "grad_norm": 0.7690190076828003,
      "learning_rate": 4.6945395674028046e-05,
      "loss": 0.0103,
      "step": 1590
    },
    {
      "epoch": 155.609756097561,
      "grad_norm": 0.33617088198661804,
      "learning_rate": 4.6926558388363906e-05,
      "loss": 0.0091,
      "step": 1595
    },
    {
      "epoch": 156.09756097560975,
      "grad_norm": 0.3891561031341553,
      "learning_rate": 4.690766700109659e-05,
      "loss": 0.0103,
      "step": 1600
    },
    {
      "epoch": 156.09756097560975,
      "eval_loss": 4.520708084106445,
      "eval_runtime": 1.8167,
      "eval_samples_per_second": 16.513,
      "eval_steps_per_second": 1.101,
      "step": 1600
    },
    {
      "epoch": 156.58536585365854,
      "grad_norm": 0.7959368824958801,
      "learning_rate": 4.688872155883873e-05,
      "loss": 0.0087,
      "step": 1605
    },
    {
      "epoch": 157.0731707317073,
      "grad_norm": 0.6259908676147461,
      "learning_rate": 4.6869722108336323e-05,
      "loss": 0.0104,
      "step": 1610
    },
    {
      "epoch": 157.5609756097561,
      "grad_norm": 0.2535293996334076,
      "learning_rate": 4.6850668696468614e-05,
      "loss": 0.0079,
      "step": 1615
    },
    {
      "epoch": 158.0487804878049,
      "grad_norm": 0.4718330204486847,
      "learning_rate": 4.683156137024801e-05,
      "loss": 0.0092,
      "step": 1620
    },
    {
      "epoch": 158.53658536585365,
      "grad_norm": 0.13067412376403809,
      "learning_rate": 4.681240017681993e-05,
      "loss": 0.0072,
      "step": 1625
    },
    {
      "epoch": 159.02439024390245,
      "grad_norm": 0.2684772312641144,
      "learning_rate": 4.6793185163462726e-05,
      "loss": 0.0084,
      "step": 1630
    },
    {
      "epoch": 159.5121951219512,
      "grad_norm": 0.08670541644096375,
      "learning_rate": 4.677391637758752e-05,
      "loss": 0.0066,
      "step": 1635
    },
    {
      "epoch": 160.0,
      "grad_norm": 0.3601645529270172,
      "learning_rate": 4.675459386673815e-05,
      "loss": 0.0078,
      "step": 1640
    },
    {
      "epoch": 160.4878048780488,
      "grad_norm": 0.07485790550708771,
      "learning_rate": 4.673521767859096e-05,
      "loss": 0.0063,
      "step": 1645
    },
    {
      "epoch": 160.97560975609755,
      "grad_norm": 0.08126981556415558,
      "learning_rate": 4.671578786095478e-05,
      "loss": 0.0062,
      "step": 1650
    },
    {
      "epoch": 161.46341463414635,
      "grad_norm": 0.0746045932173729,
      "learning_rate": 4.6696304461770765e-05,
      "loss": 0.007,
      "step": 1655
    },
    {
      "epoch": 161.9512195121951,
      "grad_norm": 0.077287457883358,
      "learning_rate": 4.667676752911225e-05,
      "loss": 0.006,
      "step": 1660
    },
    {
      "epoch": 162.4390243902439,
      "grad_norm": 0.06097447872161865,
      "learning_rate": 4.665717711118469e-05,
      "loss": 0.0066,
      "step": 1665
    },
    {
      "epoch": 162.9268292682927,
      "grad_norm": 0.058160245418548584,
      "learning_rate": 4.663753325632548e-05,
      "loss": 0.0057,
      "step": 1670
    },
    {
      "epoch": 163.41463414634146,
      "grad_norm": 0.053327251225709915,
      "learning_rate": 4.661783601300388e-05,
      "loss": 0.0063,
      "step": 1675
    },
    {
      "epoch": 163.90243902439025,
      "grad_norm": 0.053246788680553436,
      "learning_rate": 4.659808542982088e-05,
      "loss": 0.0055,
      "step": 1680
    },
    {
      "epoch": 164.390243902439,
      "grad_norm": 0.05008949711918831,
      "learning_rate": 4.65782815555091e-05,
      "loss": 0.0062,
      "step": 1685
    },
    {
      "epoch": 164.8780487804878,
      "grad_norm": 0.0526277981698513,
      "learning_rate": 4.65584244389326e-05,
      "loss": 0.0055,
      "step": 1690
    },
    {
      "epoch": 165.3658536585366,
      "grad_norm": 0.047135911881923676,
      "learning_rate": 4.6538514129086866e-05,
      "loss": 0.006,
      "step": 1695
    },
    {
      "epoch": 165.85365853658536,
      "grad_norm": 0.047617774456739426,
      "learning_rate": 4.65185506750986e-05,
      "loss": 0.0052,
      "step": 1700
    },
    {
      "epoch": 165.85365853658536,
      "eval_loss": 4.62808895111084,
      "eval_runtime": 1.8797,
      "eval_samples_per_second": 15.96,
      "eval_steps_per_second": 1.064,
      "step": 1700
    },
    {
      "epoch": 166.34146341463415,
      "grad_norm": 0.04521783068776131,
      "learning_rate": 4.649853412622563e-05,
      "loss": 0.0058,
      "step": 1705
    },
    {
      "epoch": 166.82926829268294,
      "grad_norm": 0.04955103620886803,
      "learning_rate": 4.647846453185681e-05,
      "loss": 0.0052,
      "step": 1710
    },
    {
      "epoch": 167.3170731707317,
      "grad_norm": 0.04418556019663811,
      "learning_rate": 4.645834194151187e-05,
      "loss": 0.0059,
      "step": 1715
    },
    {
      "epoch": 167.8048780487805,
      "grad_norm": 0.0449599027633667,
      "learning_rate": 4.643816640484131e-05,
      "loss": 0.005,
      "step": 1720
    },
    {
      "epoch": 168.29268292682926,
      "grad_norm": 0.04484715312719345,
      "learning_rate": 4.6417937971626245e-05,
      "loss": 0.0057,
      "step": 1725
    },
    {
      "epoch": 168.78048780487805,
      "grad_norm": 0.04517429322004318,
      "learning_rate": 4.639765669177833e-05,
      "loss": 0.0049,
      "step": 1730
    },
    {
      "epoch": 169.26829268292684,
      "grad_norm": 0.043170224875211716,
      "learning_rate": 4.63773226153396e-05,
      "loss": 0.0056,
      "step": 1735
    },
    {
      "epoch": 169.7560975609756,
      "grad_norm": 0.04417217895388603,
      "learning_rate": 4.635693579248238e-05,
      "loss": 0.0049,
      "step": 1740
    },
    {
      "epoch": 170.2439024390244,
      "grad_norm": 0.04240555316209793,
      "learning_rate": 4.633649627350912e-05,
      "loss": 0.0053,
      "step": 1745
    },
    {
      "epoch": 170.73170731707316,
      "grad_norm": 0.042470015585422516,
      "learning_rate": 4.6316004108852305e-05,
      "loss": 0.0047,
      "step": 1750
    },
    {
      "epoch": 171.21951219512195,
      "grad_norm": 0.04104406014084816,
      "learning_rate": 4.629545934907432e-05,
      "loss": 0.0054,
      "step": 1755
    },
    {
      "epoch": 171.70731707317074,
      "grad_norm": 0.04309443011879921,
      "learning_rate": 4.6274862044867304e-05,
      "loss": 0.0046,
      "step": 1760
    },
    {
      "epoch": 172.1951219512195,
      "grad_norm": 0.04003475606441498,
      "learning_rate": 4.625421224705306e-05,
      "loss": 0.0054,
      "step": 1765
    },
    {
      "epoch": 172.6829268292683,
      "grad_norm": 0.039988357573747635,
      "learning_rate": 4.6233510006582914e-05,
      "loss": 0.0045,
      "step": 1770
    },
    {
      "epoch": 173.17073170731706,
      "grad_norm": 0.03937947750091553,
      "learning_rate": 4.6212755374537596e-05,
      "loss": 0.0052,
      "step": 1775
    },
    {
      "epoch": 173.65853658536585,
      "grad_norm": 0.03830491751432419,
      "learning_rate": 4.619194840212708e-05,
      "loss": 0.0046,
      "step": 1780
    },
    {
      "epoch": 174.14634146341464,
      "grad_norm": 0.037541747093200684,
      "learning_rate": 4.617108914069052e-05,
      "loss": 0.005,
      "step": 1785
    },
    {
      "epoch": 174.6341463414634,
      "grad_norm": 0.03953609615564346,
      "learning_rate": 4.6150177641696055e-05,
      "loss": 0.0044,
      "step": 1790
    },
    {
      "epoch": 175.1219512195122,
      "grad_norm": 0.037545040249824524,
      "learning_rate": 4.6129213956740744e-05,
      "loss": 0.0051,
      "step": 1795
    },
    {
      "epoch": 175.609756097561,
      "grad_norm": 0.03808276727795601,
      "learning_rate": 4.610819813755038e-05,
      "loss": 0.0044,
      "step": 1800
    },
    {
      "epoch": 175.609756097561,
      "eval_loss": 4.704071998596191,
      "eval_runtime": 1.819,
      "eval_samples_per_second": 16.492,
      "eval_steps_per_second": 1.099,
      "step": 1800
    },
    {
      "epoch": 176.09756097560975,
      "grad_norm": 0.03803635761141777,
      "learning_rate": 4.608713023597941e-05,
      "loss": 0.0048,
      "step": 1805
    },
    {
      "epoch": 176.58536585365854,
      "grad_norm": 0.03937898948788643,
      "learning_rate": 4.606601030401081e-05,
      "loss": 0.0043,
      "step": 1810
    },
    {
      "epoch": 177.0731707317073,
      "grad_norm": 0.10143379122018814,
      "learning_rate": 4.6044838393755884e-05,
      "loss": 0.0049,
      "step": 1815
    },
    {
      "epoch": 177.5609756097561,
      "grad_norm": 0.03505697473883629,
      "learning_rate": 4.602361455745423e-05,
      "loss": 0.0042,
      "step": 1820
    },
    {
      "epoch": 178.0487804878049,
      "grad_norm": 0.08723797649145126,
      "learning_rate": 4.600233884747355e-05,
      "loss": 0.0049,
      "step": 1825
    },
    {
      "epoch": 178.53658536585365,
      "grad_norm": 0.03754905238747597,
      "learning_rate": 4.598101131630954e-05,
      "loss": 0.0042,
      "step": 1830
    },
    {
      "epoch": 179.02439024390245,
      "grad_norm": 0.09535627067089081,
      "learning_rate": 4.595963201658577e-05,
      "loss": 0.0046,
      "step": 1835
    },
    {
      "epoch": 179.5121951219512,
      "grad_norm": 0.03773348778486252,
      "learning_rate": 4.593820100105355e-05,
      "loss": 0.0041,
      "step": 1840
    },
    {
      "epoch": 180.0,
      "grad_norm": 0.09531603753566742,
      "learning_rate": 4.591671832259174e-05,
      "loss": 0.0049,
      "step": 1845
    },
    {
      "epoch": 180.4878048780488,
      "grad_norm": 0.035921018570661545,
      "learning_rate": 4.5895184034206765e-05,
      "loss": 0.004,
      "step": 1850
    },
    {
      "epoch": 180.97560975609755,
      "grad_norm": 0.03610903024673462,
      "learning_rate": 4.58735981890323e-05,
      "loss": 0.0041,
      "step": 1855
    },
    {
      "epoch": 181.46341463414635,
      "grad_norm": 0.03380611538887024,
      "learning_rate": 4.585196084032928e-05,
      "loss": 0.0047,
      "step": 1860
    },
    {
      "epoch": 181.9512195121951,
      "grad_norm": 0.036261916160583496,
      "learning_rate": 4.5830272041485726e-05,
      "loss": 0.004,
      "step": 1865
    },
    {
      "epoch": 182.4390243902439,
      "grad_norm": 0.03460385277867317,
      "learning_rate": 4.580853184601659e-05,
      "loss": 0.0045,
      "step": 1870
    },
    {
      "epoch": 182.9268292682927,
      "grad_norm": 0.03595682233572006,
      "learning_rate": 4.5786740307563636e-05,
      "loss": 0.004,
      "step": 1875
    },
    {
      "epoch": 183.41463414634146,
      "grad_norm": 0.03406563773751259,
      "learning_rate": 4.5764897479895317e-05,
      "loss": 0.0044,
      "step": 1880
    },
    {
      "epoch": 183.90243902439025,
      "grad_norm": 0.03613154962658882,
      "learning_rate": 4.574300341690665e-05,
      "loss": 0.0039,
      "step": 1885
    },
    {
      "epoch": 184.390243902439,
      "grad_norm": 0.03401387855410576,
      "learning_rate": 4.572105817261905e-05,
      "loss": 0.0045,
      "step": 1890
    },
    {
      "epoch": 184.8780487804878,
      "grad_norm": 0.035506702959537506,
      "learning_rate": 4.569906180118023e-05,
      "loss": 0.0038,
      "step": 1895
    },
    {
      "epoch": 185.3658536585366,
      "grad_norm": 0.03548724204301834,
      "learning_rate": 4.567701435686404e-05,
      "loss": 0.0044,
      "step": 1900
    },
    {
      "epoch": 185.3658536585366,
      "eval_loss": 4.789017200469971,
      "eval_runtime": 2.002,
      "eval_samples_per_second": 14.985,
      "eval_steps_per_second": 0.999,
      "step": 1900
    },
    {
      "epoch": 185.85365853658536,
      "grad_norm": 0.03399983048439026,
      "learning_rate": 4.5654915894070384e-05,
      "loss": 0.0038,
      "step": 1905
    },
    {
      "epoch": 186.34146341463415,
      "grad_norm": 0.03391556069254875,
      "learning_rate": 4.563276646732499e-05,
      "loss": 0.0044,
      "step": 1910
    },
    {
      "epoch": 186.82926829268294,
      "grad_norm": 0.032097723335027695,
      "learning_rate": 4.561056613127939e-05,
      "loss": 0.0037,
      "step": 1915
    },
    {
      "epoch": 187.3170731707317,
      "grad_norm": 0.03165567293763161,
      "learning_rate": 4.558831494071069e-05,
      "loss": 0.0043,
      "step": 1920
    },
    {
      "epoch": 187.8048780487805,
      "grad_norm": 0.03233035281300545,
      "learning_rate": 4.55660129505215e-05,
      "loss": 0.0037,
      "step": 1925
    },
    {
      "epoch": 188.29268292682926,
      "grad_norm": 0.03444575145840645,
      "learning_rate": 4.554366021573976e-05,
      "loss": 0.0042,
      "step": 1930
    },
    {
      "epoch": 188.78048780487805,
      "grad_norm": 0.0327535979449749,
      "learning_rate": 4.552125679151862e-05,
      "loss": 0.0037,
      "step": 1935
    },
    {
      "epoch": 189.26829268292684,
      "grad_norm": 0.033521704375743866,
      "learning_rate": 4.549880273313631e-05,
      "loss": 0.0042,
      "step": 1940
    },
    {
      "epoch": 189.7560975609756,
      "grad_norm": 0.03306612744927406,
      "learning_rate": 4.5476298095995984e-05,
      "loss": 0.0035,
      "step": 1945
    },
    {
      "epoch": 190.2439024390244,
      "grad_norm": 0.03159721568226814,
      "learning_rate": 4.545374293562559e-05,
      "loss": 0.0041,
      "step": 1950
    },
    {
      "epoch": 190.73170731707316,
      "grad_norm": 0.03197261691093445,
      "learning_rate": 4.543113730767775e-05,
      "loss": 0.0036,
      "step": 1955
    },
    {
      "epoch": 191.21951219512195,
      "grad_norm": 0.03188400715589523,
      "learning_rate": 4.5408481267929605e-05,
      "loss": 0.0041,
      "step": 1960
    },
    {
      "epoch": 191.70731707317074,
      "grad_norm": 0.03223150223493576,
      "learning_rate": 4.538577487228267e-05,
      "loss": 0.0035,
      "step": 1965
    },
    {
      "epoch": 192.1951219512195,
      "grad_norm": 0.030442385002970695,
      "learning_rate": 4.536301817676274e-05,
      "loss": 0.004,
      "step": 1970
    },
    {
      "epoch": 192.6829268292683,
      "grad_norm": 0.03229638561606407,
      "learning_rate": 4.534021123751968e-05,
      "loss": 0.0034,
      "step": 1975
    },
    {
      "epoch": 193.17073170731706,
      "grad_norm": 0.032370083034038544,
      "learning_rate": 4.531735411082735e-05,
      "loss": 0.004,
      "step": 1980
    },
    {
      "epoch": 193.65853658536585,
      "grad_norm": 0.03299262002110481,
      "learning_rate": 4.529444685308344e-05,
      "loss": 0.0035,
      "step": 1985
    },
    {
      "epoch": 194.14634146341464,
      "grad_norm": 0.029695695266127586,
      "learning_rate": 4.527148952080934e-05,
      "loss": 0.0039,
      "step": 1990
    },
    {
      "epoch": 194.6341463414634,
      "grad_norm": 0.02989080548286438,
      "learning_rate": 4.524848217064997e-05,
      "loss": 0.0033,
      "step": 1995
    },
    {
      "epoch": 195.1219512195122,
      "grad_norm": 0.029994146898388863,
      "learning_rate": 4.522542485937369e-05,
      "loss": 0.004,
      "step": 2000
    },
    {
      "epoch": 195.1219512195122,
      "eval_loss": 4.83812141418457,
      "eval_runtime": 1.9827,
      "eval_samples_per_second": 15.131,
      "eval_steps_per_second": 1.009,
      "step": 2000
    },
    {
      "epoch": 195.609756097561,
      "grad_norm": 0.030861496925354004,
      "learning_rate": 4.5202317643872114e-05,
      "loss": 0.0034,
      "step": 2005
    },
    {
      "epoch": 196.09756097560975,
      "grad_norm": 0.029693344607949257,
      "learning_rate": 4.5179160581160005e-05,
      "loss": 0.0039,
      "step": 2010
    },
    {
      "epoch": 196.58536585365854,
      "grad_norm": 0.031240513548254967,
      "learning_rate": 4.515595372837512e-05,
      "loss": 0.0033,
      "step": 2015
    },
    {
      "epoch": 197.0731707317073,
      "grad_norm": 0.07159976661205292,
      "learning_rate": 4.513269714277805e-05,
      "loss": 0.0038,
      "step": 2020
    },
    {
      "epoch": 197.5609756097561,
      "grad_norm": 0.0312609039247036,
      "learning_rate": 4.5109390881752114e-05,
      "loss": 0.0033,
      "step": 2025
    },
    {
      "epoch": 198.0487804878049,
      "grad_norm": 0.07741883397102356,
      "learning_rate": 4.5086035002803195e-05,
      "loss": 0.0038,
      "step": 2030
    },
    {
      "epoch": 198.53658536585365,
      "grad_norm": 0.0287924874573946,
      "learning_rate": 4.506262956355959e-05,
      "loss": 0.0032,
      "step": 2035
    },
    {
      "epoch": 199.02439024390245,
      "grad_norm": 0.07905016839504242,
      "learning_rate": 4.503917462177192e-05,
      "loss": 0.0038,
      "step": 2040
    },
    {
      "epoch": 199.5121951219512,
      "grad_norm": 0.030044948682188988,
      "learning_rate": 4.5015670235312896e-05,
      "loss": 0.0032,
      "step": 2045
    },
    {
      "epoch": 200.0,
      "grad_norm": 0.07376820594072342,
      "learning_rate": 4.499211646217727e-05,
      "loss": 0.0037,
      "step": 2050
    },
    {
      "epoch": 200.4878048780488,
      "grad_norm": 0.02903175912797451,
      "learning_rate": 4.496851336048162e-05,
      "loss": 0.0032,
      "step": 2055
    },
    {
      "epoch": 200.97560975609755,
      "grad_norm": 0.02860807441174984,
      "learning_rate": 4.4944860988464276e-05,
      "loss": 0.0031,
      "step": 2060
    },
    {
      "epoch": 201.46341463414635,
      "grad_norm": 0.02695772983133793,
      "learning_rate": 4.49211594044851e-05,
      "loss": 0.0037,
      "step": 2065
    },
    {
      "epoch": 201.9512195121951,
      "grad_norm": 0.028108354657888412,
      "learning_rate": 4.48974086670254e-05,
      "loss": 0.0031,
      "step": 2070
    },
    {
      "epoch": 202.4390243902439,
      "grad_norm": 0.028042741119861603,
      "learning_rate": 4.487360883468775e-05,
      "loss": 0.0036,
      "step": 2075
    },
    {
      "epoch": 202.9268292682927,
      "grad_norm": 0.027652272954583168,
      "learning_rate": 4.484975996619589e-05,
      "loss": 0.0031,
      "step": 2080
    },
    {
      "epoch": 203.41463414634146,
      "grad_norm": 0.027178851887583733,
      "learning_rate": 4.482586212039451e-05,
      "loss": 0.0036,
      "step": 2085
    },
    {
      "epoch": 203.90243902439025,
      "grad_norm": 0.027563683688640594,
      "learning_rate": 4.480191535624918e-05,
      "loss": 0.0031,
      "step": 2090
    },
    {
      "epoch": 204.390243902439,
      "grad_norm": 0.02749893069267273,
      "learning_rate": 4.4777919732846166e-05,
      "loss": 0.0035,
      "step": 2095
    },
    {
      "epoch": 204.8780487804878,
      "grad_norm": 0.02821865677833557,
      "learning_rate": 4.4753875309392266e-05,
      "loss": 0.0031,
      "step": 2100
    },
    {
      "epoch": 204.8780487804878,
      "eval_loss": 4.895698547363281,
      "eval_runtime": 2.0002,
      "eval_samples_per_second": 14.998,
      "eval_steps_per_second": 1.0,
      "step": 2100
    },
    {
      "epoch": 205.3658536585366,
      "grad_norm": 0.028037307783961296,
      "learning_rate": 4.4729782145214716e-05,
      "loss": 0.0035,
      "step": 2105
    },
    {
      "epoch": 205.85365853658536,
      "grad_norm": 0.02794197015464306,
      "learning_rate": 4.4705640299761007e-05,
      "loss": 0.003,
      "step": 2110
    },
    {
      "epoch": 206.34146341463415,
      "grad_norm": 0.026436759158968925,
      "learning_rate": 4.468144983259873e-05,
      "loss": 0.0034,
      "step": 2115
    },
    {
      "epoch": 206.82926829268294,
      "grad_norm": 0.026345055550336838,
      "learning_rate": 4.465721080341547e-05,
      "loss": 0.003,
      "step": 2120
    },
    {
      "epoch": 207.3170731707317,
      "grad_norm": 0.024698808789253235,
      "learning_rate": 4.463292327201862e-05,
      "loss": 0.0034,
      "step": 2125
    },
    {
      "epoch": 207.8048780487805,
      "grad_norm": 0.026290686801075935,
      "learning_rate": 4.460858729833525e-05,
      "loss": 0.0029,
      "step": 2130
    },
    {
      "epoch": 208.29268292682926,
      "grad_norm": 0.024930668994784355,
      "learning_rate": 4.458420294241196e-05,
      "loss": 0.0034,
      "step": 2135
    },
    {
      "epoch": 208.78048780487805,
      "grad_norm": 0.025429068133234978,
      "learning_rate": 4.45597702644147e-05,
      "loss": 0.0029,
      "step": 2140
    },
    {
      "epoch": 209.26829268292684,
      "grad_norm": 0.027022548019886017,
      "learning_rate": 4.45352893246287e-05,
      "loss": 0.0034,
      "step": 2145
    },
    {
      "epoch": 209.7560975609756,
      "grad_norm": 0.028142303228378296,
      "learning_rate": 4.451076018345825e-05,
      "loss": 0.0029,
      "step": 2150
    },
    {
      "epoch": 210.2439024390244,
      "grad_norm": 0.025038491934537888,
      "learning_rate": 4.448618290142654e-05,
      "loss": 0.0034,
      "step": 2155
    },
    {
      "epoch": 210.73170731707316,
      "grad_norm": 0.024927454069256783,
      "learning_rate": 4.4461557539175594e-05,
      "loss": 0.0029,
      "step": 2160
    },
    {
      "epoch": 211.21951219512195,
      "grad_norm": 0.0260861087590456,
      "learning_rate": 4.4436884157466025e-05,
      "loss": 0.0032,
      "step": 2165
    },
    {
      "epoch": 211.70731707317074,
      "grad_norm": 0.025681784376502037,
      "learning_rate": 4.4412162817176965e-05,
      "loss": 0.0028,
      "step": 2170
    },
    {
      "epoch": 212.1951219512195,
      "grad_norm": 0.02450300194323063,
      "learning_rate": 4.4387393579305865e-05,
      "loss": 0.0033,
      "step": 2175
    },
    {
      "epoch": 212.6829268292683,
      "grad_norm": 0.02612573467195034,
      "learning_rate": 4.436257650496834e-05,
      "loss": 0.0028,
      "step": 2180
    },
    {
      "epoch": 213.17073170731706,
      "grad_norm": 0.024202236905694008,
      "learning_rate": 4.433771165539808e-05,
      "loss": 0.0033,
      "step": 2185
    },
    {
      "epoch": 213.65853658536585,
      "grad_norm": 0.026124808937311172,
      "learning_rate": 4.431279909194661e-05,
      "loss": 0.0028,
      "step": 2190
    },
    {
      "epoch": 214.14634146341464,
      "grad_norm": 0.024214882403612137,
      "learning_rate": 4.428783887608321e-05,
      "loss": 0.0032,
      "step": 2195
    },
    {
      "epoch": 214.6341463414634,
      "grad_norm": 0.02540711686015129,
      "learning_rate": 4.426283106939474e-05,
      "loss": 0.0028,
      "step": 2200
    },
    {
      "epoch": 214.6341463414634,
      "eval_loss": 4.939047813415527,
      "eval_runtime": 2.0018,
      "eval_samples_per_second": 14.986,
      "eval_steps_per_second": 0.999,
      "step": 2200
    },
    {
      "epoch": 215.1219512195122,
      "grad_norm": 0.024114057421684265,
      "learning_rate": 4.423777573358545e-05,
      "loss": 0.0031,
      "step": 2205
    },
    {
      "epoch": 215.609756097561,
      "grad_norm": 0.02612341195344925,
      "learning_rate": 4.4212672930476915e-05,
      "loss": 0.0028,
      "step": 2210
    },
    {
      "epoch": 216.09756097560975,
      "grad_norm": 0.02388370782136917,
      "learning_rate": 4.4187522722007805e-05,
      "loss": 0.0032,
      "step": 2215
    },
    {
      "epoch": 216.58536585365854,
      "grad_norm": 0.025275595486164093,
      "learning_rate": 4.4162325170233745e-05,
      "loss": 0.0027,
      "step": 2220
    },
    {
      "epoch": 217.0731707317073,
      "grad_norm": 0.06771475076675415,
      "learning_rate": 4.4137080337327205e-05,
      "loss": 0.0031,
      "step": 2225
    },
    {
      "epoch": 217.5609756097561,
      "grad_norm": 0.02397923357784748,
      "learning_rate": 4.4111788285577294e-05,
      "loss": 0.0026,
      "step": 2230
    },
    {
      "epoch": 218.0487804878049,
      "grad_norm": 0.06082369387149811,
      "learning_rate": 4.408644907738964e-05,
      "loss": 0.0031,
      "step": 2235
    },
    {
      "epoch": 218.53658536585365,
      "grad_norm": 0.023002643138170242,
      "learning_rate": 4.40610627752862e-05,
      "loss": 0.0027,
      "step": 2240
    },
    {
      "epoch": 219.02439024390245,
      "grad_norm": 0.05893978476524353,
      "learning_rate": 4.4035629441905174e-05,
      "loss": 0.003,
      "step": 2245
    },
    {
      "epoch": 219.5121951219512,
      "grad_norm": 0.02291400544345379,
      "learning_rate": 4.401014914000078e-05,
      "loss": 0.0026,
      "step": 2250
    },
    {
      "epoch": 220.0,
      "grad_norm": 0.057309940457344055,
      "learning_rate": 4.398462193244312e-05,
      "loss": 0.003,
      "step": 2255
    },
    {
      "epoch": 220.4878048780488,
      "grad_norm": 0.0246867798268795,
      "learning_rate": 4.395904788221805e-05,
      "loss": 0.0026,
      "step": 2260
    },
    {
      "epoch": 220.97560975609755,
      "grad_norm": 0.023770449683070183,
      "learning_rate": 4.393342705242699e-05,
      "loss": 0.0026,
      "step": 2265
    },
    {
      "epoch": 221.46341463414635,
      "grad_norm": 0.02361590601503849,
      "learning_rate": 4.39077595062868e-05,
      "loss": 0.003,
      "step": 2270
    },
    {
      "epoch": 221.9512195121951,
      "grad_norm": 0.02490297332406044,
      "learning_rate": 4.3882045307129594e-05,
      "loss": 0.0026,
      "step": 2275
    },
    {
      "epoch": 222.4390243902439,
      "grad_norm": 0.02489425428211689,
      "learning_rate": 4.3856284518402594e-05,
      "loss": 0.0029,
      "step": 2280
    },
    {
      "epoch": 222.9268292682927,
      "grad_norm": 0.02388874627649784,
      "learning_rate": 4.3830477203668005e-05,
      "loss": 0.0026,
      "step": 2285
    },
    {
      "epoch": 223.41463414634146,
      "grad_norm": 0.023213699460029602,
      "learning_rate": 4.3804623426602784e-05,
      "loss": 0.003,
      "step": 2290
    },
    {
      "epoch": 223.90243902439025,
      "grad_norm": 0.022923383861780167,
      "learning_rate": 4.377872325099858e-05,
      "loss": 0.0025,
      "step": 2295
    },
    {
      "epoch": 224.390243902439,
      "grad_norm": 0.02199867181479931,
      "learning_rate": 4.375277674076149e-05,
      "loss": 0.0029,
      "step": 2300
    },
    {
      "epoch": 224.390243902439,
      "eval_loss": 4.98386812210083,
      "eval_runtime": 2.0837,
      "eval_samples_per_second": 14.398,
      "eval_steps_per_second": 0.96,
      "step": 2300
    },
    {
      "epoch": 224.8780487804878,
      "grad_norm": 0.02302994579076767,
      "learning_rate": 4.3726783959911956e-05,
      "loss": 0.0025,
      "step": 2305
    },
    {
      "epoch": 225.3658536585366,
      "grad_norm": 0.022918833419680595,
      "learning_rate": 4.370074497258456e-05,
      "loss": 0.0029,
      "step": 2310
    },
    {
      "epoch": 225.85365853658536,
      "grad_norm": 0.024179458618164062,
      "learning_rate": 4.367465984302794e-05,
      "loss": 0.0025,
      "step": 2315
    },
    {
      "epoch": 226.34146341463415,
      "grad_norm": 0.022544201463460922,
      "learning_rate": 4.3648528635604556e-05,
      "loss": 0.0029,
      "step": 2320
    },
    {
      "epoch": 226.82926829268294,
      "grad_norm": 0.023896221071481705,
      "learning_rate": 4.3622351414790554e-05,
      "loss": 0.0025,
      "step": 2325
    },
    {
      "epoch": 227.3170731707317,
      "grad_norm": 0.02310146950185299,
      "learning_rate": 4.359612824517563e-05,
      "loss": 0.0028,
      "step": 2330
    },
    {
      "epoch": 227.8048780487805,
      "grad_norm": 0.022988364100456238,
      "learning_rate": 4.3569859191462845e-05,
      "loss": 0.0024,
      "step": 2335
    },
    {
      "epoch": 228.29268292682926,
      "grad_norm": 0.02208635024726391,
      "learning_rate": 4.3543544318468485e-05,
      "loss": 0.0028,
      "step": 2340
    },
    {
      "epoch": 228.78048780487805,
      "grad_norm": 0.0224353838711977,
      "learning_rate": 4.3517183691121874e-05,
      "loss": 0.0024,
      "step": 2345
    },
    {
      "epoch": 229.26829268292684,
      "grad_norm": 0.023184238001704216,
      "learning_rate": 4.349077737446525e-05,
      "loss": 0.0028,
      "step": 2350
    },
    {
      "epoch": 229.7560975609756,
      "grad_norm": 0.021265164017677307,
      "learning_rate": 4.3464325433653566e-05,
      "loss": 0.0024,
      "step": 2355
    },
    {
      "epoch": 230.2439024390244,
      "grad_norm": 0.022364450618624687,
      "learning_rate": 4.343782793395435e-05,
      "loss": 0.0028,
      "step": 2360
    },
    {
      "epoch": 230.73170731707316,
      "grad_norm": 0.02367018721997738,
      "learning_rate": 4.3411284940747566e-05,
      "loss": 0.0024,
      "step": 2365
    },
    {
      "epoch": 231.21951219512195,
      "grad_norm": 0.02246522530913353,
      "learning_rate": 4.33846965195254e-05,
      "loss": 0.0027,
      "step": 2370
    },
    {
      "epoch": 231.70731707317074,
      "grad_norm": 0.02311052195727825,
      "learning_rate": 4.335806273589214e-05,
      "loss": 0.0024,
      "step": 2375
    },
    {
      "epoch": 232.1951219512195,
      "grad_norm": 0.021678201854228973,
      "learning_rate": 4.3331383655564006e-05,
      "loss": 0.0027,
      "step": 2380
    },
    {
      "epoch": 232.6829268292683,
      "grad_norm": 0.022818943485617638,
      "learning_rate": 4.330465934436896e-05,
      "loss": 0.0023,
      "step": 2385
    },
    {
      "epoch": 233.17073170731706,
      "grad_norm": 0.02084946446120739,
      "learning_rate": 4.327788986824661e-05,
      "loss": 0.0027,
      "step": 2390
    },
    {
      "epoch": 233.65853658536585,
      "grad_norm": 0.02165311574935913,
      "learning_rate": 4.325107529324795e-05,
      "loss": 0.0023,
      "step": 2395
    },
    {
      "epoch": 234.14634146341464,
      "grad_norm": 0.02164989709854126,
      "learning_rate": 4.3224215685535294e-05,
      "loss": 0.0027,
      "step": 2400
    },
    {
      "epoch": 234.14634146341464,
      "eval_loss": 5.052450180053711,
      "eval_runtime": 2.0921,
      "eval_samples_per_second": 14.34,
      "eval_steps_per_second": 0.956,
      "step": 2400
    },
    {
      "epoch": 234.6341463414634,
      "grad_norm": 0.021085672080516815,
      "learning_rate": 4.3197311111382045e-05,
      "loss": 0.0023,
      "step": 2405
    },
    {
      "epoch": 235.1219512195122,
      "grad_norm": 0.020021462813019753,
      "learning_rate": 4.3170361637172575e-05,
      "loss": 0.0027,
      "step": 2410
    },
    {
      "epoch": 235.609756097561,
      "grad_norm": 0.02101912535727024,
      "learning_rate": 4.314336732940202e-05,
      "loss": 0.0023,
      "step": 2415
    },
    {
      "epoch": 236.09756097560975,
      "grad_norm": 0.02079058066010475,
      "learning_rate": 4.311632825467617e-05,
      "loss": 0.0027,
      "step": 2420
    },
    {
      "epoch": 236.58536585365854,
      "grad_norm": 0.02083275280892849,
      "learning_rate": 4.3089244479711236e-05,
      "loss": 0.0023,
      "step": 2425
    },
    {
      "epoch": 237.0731707317073,
      "grad_norm": 0.050872258841991425,
      "learning_rate": 4.3062116071333746e-05,
      "loss": 0.0026,
      "step": 2430
    },
    {
      "epoch": 237.5609756097561,
      "grad_norm": 0.01987280882894993,
      "learning_rate": 4.3034943096480354e-05,
      "loss": 0.0022,
      "step": 2435
    },
    {
      "epoch": 238.0487804878049,
      "grad_norm": 0.05219472572207451,
      "learning_rate": 4.3007725622197674e-05,
      "loss": 0.0026,
      "step": 2440
    },
    {
      "epoch": 238.53658536585365,
      "grad_norm": 0.018841274082660675,
      "learning_rate": 4.2980463715642116e-05,
      "loss": 0.0022,
      "step": 2445
    },
    {
      "epoch": 239.02439024390245,
      "grad_norm": 0.05132045969367027,
      "learning_rate": 4.295315744407972e-05,
      "loss": 0.0026,
      "step": 2450
    },
    {
      "epoch": 239.5121951219512,
      "grad_norm": 0.02075289562344551,
      "learning_rate": 4.292580687488601e-05,
      "loss": 0.0022,
      "step": 2455
    },
    {
      "epoch": 240.0,
      "grad_norm": 0.0489749051630497,
      "learning_rate": 4.289841207554578e-05,
      "loss": 0.0025,
      "step": 2460
    },
    {
      "epoch": 240.4878048780488,
      "grad_norm": 0.022373922169208527,
      "learning_rate": 4.287097311365299e-05,
      "loss": 0.0022,
      "step": 2465
    },
    {
      "epoch": 240.97560975609755,
      "grad_norm": 0.019579851999878883,
      "learning_rate": 4.2843490056910534e-05,
      "loss": 0.0022,
      "step": 2470
    },
    {
      "epoch": 241.46341463414635,
      "grad_norm": 0.020672878250479698,
      "learning_rate": 4.281596297313013e-05,
      "loss": 0.0025,
      "step": 2475
    },
    {
      "epoch": 241.9512195121951,
      "grad_norm": 0.019761331379413605,
      "learning_rate": 4.278839193023214e-05,
      "loss": 0.0022,
      "step": 2480
    },
    {
      "epoch": 242.4390243902439,
      "grad_norm": 0.018908781930804253,
      "learning_rate": 4.2760776996245336e-05,
      "loss": 0.0025,
      "step": 2485
    },
    {
      "epoch": 242.9268292682927,
      "grad_norm": 0.02056863158941269,
      "learning_rate": 4.273311823930685e-05,
      "loss": 0.0021,
      "step": 2490
    },
    {
      "epoch": 243.41463414634146,
      "grad_norm": 0.018812211230397224,
      "learning_rate": 4.27054157276619e-05,
      "loss": 0.0024,
      "step": 2495
    },
    {
      "epoch": 243.90243902439025,
      "grad_norm": 0.019218452274799347,
      "learning_rate": 4.267766952966369e-05,
      "loss": 0.0022,
      "step": 2500
    },
    {
      "epoch": 243.90243902439025,
      "eval_loss": 5.076612949371338,
      "eval_runtime": 2.001,
      "eval_samples_per_second": 14.993,
      "eval_steps_per_second": 1.0,
      "step": 2500
    },
    {
      "epoch": 244.390243902439,
      "grad_norm": 0.01961027830839157,
      "learning_rate": 4.26498797137732e-05,
      "loss": 0.0024,
      "step": 2505
    },
    {
      "epoch": 244.8780487804878,
      "grad_norm": 0.019610261544585228,
      "learning_rate": 4.262204634855904e-05,
      "loss": 0.0021,
      "step": 2510
    },
    {
      "epoch": 245.3658536585366,
      "grad_norm": 0.01972811296582222,
      "learning_rate": 4.259416950269727e-05,
      "loss": 0.0025,
      "step": 2515
    },
    {
      "epoch": 245.85365853658536,
      "grad_norm": 0.01907322369515896,
      "learning_rate": 4.256624924497123e-05,
      "loss": 0.0021,
      "step": 2520
    },
    {
      "epoch": 246.34146341463415,
      "grad_norm": 0.019680486992001534,
      "learning_rate": 4.25382856442714e-05,
      "loss": 0.0024,
      "step": 2525
    },
    {
      "epoch": 246.82926829268294,
      "grad_norm": 0.01847454160451889,
      "learning_rate": 4.251027876959516e-05,
      "loss": 0.0021,
      "step": 2530
    },
    {
      "epoch": 247.3170731707317,
      "grad_norm": 0.018920529633760452,
      "learning_rate": 4.248222869004671e-05,
      "loss": 0.0024,
      "step": 2535
    },
    {
      "epoch": 247.8048780487805,
      "grad_norm": 0.01926121488213539,
      "learning_rate": 4.245413547483682e-05,
      "loss": 0.0021,
      "step": 2540
    },
    {
      "epoch": 248.29268292682926,
      "grad_norm": 0.01841096580028534,
      "learning_rate": 4.2425999193282714e-05,
      "loss": 0.0023,
      "step": 2545
    },
    {
      "epoch": 248.78048780487805,
      "grad_norm": 0.01899036020040512,
      "learning_rate": 4.2397819914807856e-05,
      "loss": 0.0021,
      "step": 2550
    },
    {
      "epoch": 249.26829268292684,
      "grad_norm": 0.019507940858602524,
      "learning_rate": 4.236959770894183e-05,
      "loss": 0.0024,
      "step": 2555
    },
    {
      "epoch": 249.7560975609756,
      "grad_norm": 0.01782262697815895,
      "learning_rate": 4.234133264532012e-05,
      "loss": 0.002,
      "step": 2560
    },
    {
      "epoch": 250.2439024390244,
      "grad_norm": 0.017852306365966797,
      "learning_rate": 4.2313024793683965e-05,
      "loss": 0.0024,
      "step": 2565
    },
    {
      "epoch": 250.73170731707316,
      "grad_norm": 0.017840631306171417,
      "learning_rate": 4.228467422388016e-05,
      "loss": 0.002,
      "step": 2570
    },
    {
      "epoch": 251.21951219512195,
      "grad_norm": 0.018728865310549736,
      "learning_rate": 4.225628100586093e-05,
      "loss": 0.0023,
      "step": 2575
    },
    {
      "epoch": 251.70731707317074,
      "grad_norm": 0.019541189074516296,
      "learning_rate": 4.2227845209683716e-05,
      "loss": 0.002,
      "step": 2580
    },
    {
      "epoch": 252.1951219512195,
      "grad_norm": 0.01787278614938259,
      "learning_rate": 4.219936690551101e-05,
      "loss": 0.0023,
      "step": 2585
    },
    {
      "epoch": 252.6829268292683,
      "grad_norm": 0.018115825951099396,
      "learning_rate": 4.217084616361021e-05,
      "loss": 0.002,
      "step": 2590
    },
    {
      "epoch": 253.17073170731706,
      "grad_norm": 0.018294868990778923,
      "learning_rate": 4.21422830543534e-05,
      "loss": 0.0023,
      "step": 2595
    },
    {
      "epoch": 253.65853658536585,
      "grad_norm": 0.018547585234045982,
      "learning_rate": 4.211367764821722e-05,
      "loss": 0.002,
      "step": 2600
    },
    {
      "epoch": 253.65853658536585,
      "eval_loss": 5.12641716003418,
      "eval_runtime": 1.9997,
      "eval_samples_per_second": 15.002,
      "eval_steps_per_second": 1.0,
      "step": 2600
    },
    {
      "epoch": 254.14634146341464,
      "grad_norm": 0.01764567382633686,
      "learning_rate": 4.208503001578266e-05,
      "loss": 0.0022,
      "step": 2605
    },
    {
      "epoch": 254.6341463414634,
      "grad_norm": 0.018165800720453262,
      "learning_rate": 4.205634022773491e-05,
      "loss": 0.002,
      "step": 2610
    },
    {
      "epoch": 255.1219512195122,
      "grad_norm": 0.018052253872156143,
      "learning_rate": 4.202760835486317e-05,
      "loss": 0.0022,
      "step": 2615
    },
    {
      "epoch": 255.609756097561,
      "grad_norm": 0.018602322787046432,
      "learning_rate": 4.199883446806048e-05,
      "loss": 0.002,
      "step": 2620
    },
    {
      "epoch": 256.0975609756098,
      "grad_norm": 0.017765721306204796,
      "learning_rate": 4.197001863832355e-05,
      "loss": 0.0022,
      "step": 2625
    },
    {
      "epoch": 256.5853658536585,
      "grad_norm": 0.017898717895150185,
      "learning_rate": 4.194116093675256e-05,
      "loss": 0.0019,
      "step": 2630
    },
    {
      "epoch": 257.0731707317073,
      "grad_norm": 0.04815424233675003,
      "learning_rate": 4.191226143455103e-05,
      "loss": 0.0023,
      "step": 2635
    },
    {
      "epoch": 257.5609756097561,
      "grad_norm": 0.01819295808672905,
      "learning_rate": 4.188332020302561e-05,
      "loss": 0.0019,
      "step": 2640
    },
    {
      "epoch": 258.0487804878049,
      "grad_norm": 0.042218420654535294,
      "learning_rate": 4.185433731358591e-05,
      "loss": 0.0023,
      "step": 2645
    },
    {
      "epoch": 258.5365853658537,
      "grad_norm": 0.017868516966700554,
      "learning_rate": 4.182531283774434e-05,
      "loss": 0.0019,
      "step": 2650
    },
    {
      "epoch": 259.0243902439024,
      "grad_norm": 0.04615341126918793,
      "learning_rate": 4.1796246847115886e-05,
      "loss": 0.0022,
      "step": 2655
    },
    {
      "epoch": 259.5121951219512,
      "grad_norm": 0.01671876758337021,
      "learning_rate": 4.1767139413418e-05,
      "loss": 0.0019,
      "step": 2660
    },
    {
      "epoch": 260.0,
      "grad_norm": 0.04496674984693527,
      "learning_rate": 4.173799060847039e-05,
      "loss": 0.0022,
      "step": 2665
    },
    {
      "epoch": 260.4878048780488,
      "grad_norm": 0.01858595386147499,
      "learning_rate": 4.1708800504194827e-05,
      "loss": 0.0019,
      "step": 2670
    },
    {
      "epoch": 260.9756097560976,
      "grad_norm": 0.018708061426877975,
      "learning_rate": 4.1679569172614996e-05,
      "loss": 0.0019,
      "step": 2675
    },
    {
      "epoch": 261.4634146341463,
      "grad_norm": 0.017305059358477592,
      "learning_rate": 4.165029668585629e-05,
      "loss": 0.0022,
      "step": 2680
    },
    {
      "epoch": 261.9512195121951,
      "grad_norm": 0.018551120534539223,
      "learning_rate": 4.162098311614567e-05,
      "loss": 0.0018,
      "step": 2685
    },
    {
      "epoch": 262.4390243902439,
      "grad_norm": 0.017244338989257812,
      "learning_rate": 4.159162853581147e-05,
      "loss": 0.0022,
      "step": 2690
    },
    {
      "epoch": 262.9268292682927,
      "grad_norm": 0.017354458570480347,
      "learning_rate": 4.156223301728316e-05,
      "loss": 0.0018,
      "step": 2695
    },
    {
      "epoch": 263.4146341463415,
      "grad_norm": 0.01685243472456932,
      "learning_rate": 4.1532796633091296e-05,
      "loss": 0.0021,
      "step": 2700
    },
    {
      "epoch": 263.4146341463415,
      "eval_loss": 5.17207145690918,
      "eval_runtime": 2.0917,
      "eval_samples_per_second": 14.342,
      "eval_steps_per_second": 0.956,
      "step": 2700
    },
    {
      "epoch": 263.9024390243902,
      "grad_norm": 0.01684068702161312,
      "learning_rate": 4.1503319455867215e-05,
      "loss": 0.0018,
      "step": 2705
    },
    {
      "epoch": 264.390243902439,
      "grad_norm": 0.016661163419485092,
      "learning_rate": 4.147380155834293e-05,
      "loss": 0.0021,
      "step": 2710
    },
    {
      "epoch": 264.8780487804878,
      "grad_norm": 0.01667027547955513,
      "learning_rate": 4.14442430133509e-05,
      "loss": 0.0018,
      "step": 2715
    },
    {
      "epoch": 265.3658536585366,
      "grad_norm": 0.017530066892504692,
      "learning_rate": 4.1414643893823914e-05,
      "loss": 0.0021,
      "step": 2720
    },
    {
      "epoch": 265.8536585365854,
      "grad_norm": 0.017433222383260727,
      "learning_rate": 4.138500427279485e-05,
      "loss": 0.0018,
      "step": 2725
    },
    {
      "epoch": 266.3414634146341,
      "grad_norm": 0.016338422894477844,
      "learning_rate": 4.135532422339653e-05,
      "loss": 0.0021,
      "step": 2730
    },
    {
      "epoch": 266.8292682926829,
      "grad_norm": 0.01694977842271328,
      "learning_rate": 4.132560381886152e-05,
      "loss": 0.0018,
      "step": 2735
    },
    {
      "epoch": 267.3170731707317,
      "grad_norm": 0.016027439385652542,
      "learning_rate": 4.1295843132521973e-05,
      "loss": 0.002,
      "step": 2740
    },
    {
      "epoch": 267.8048780487805,
      "grad_norm": 0.01660786382853985,
      "learning_rate": 4.126604223780941e-05,
      "loss": 0.0018,
      "step": 2745
    },
    {
      "epoch": 268.2926829268293,
      "grad_norm": 0.015946635976433754,
      "learning_rate": 4.123620120825459e-05,
      "loss": 0.002,
      "step": 2750
    },
    {
      "epoch": 268.780487804878,
      "grad_norm": 0.017441939562559128,
      "learning_rate": 4.1206320117487285e-05,
      "loss": 0.0018,
      "step": 2755
    },
    {
      "epoch": 269.2682926829268,
      "grad_norm": 0.015606405213475227,
      "learning_rate": 4.1176399039236116e-05,
      "loss": 0.002,
      "step": 2760
    },
    {
      "epoch": 269.7560975609756,
      "grad_norm": 0.016887253150343895,
      "learning_rate": 4.114643804732835e-05,
      "loss": 0.0017,
      "step": 2765
    },
    {
      "epoch": 270.2439024390244,
      "grad_norm": 0.015607266686856747,
      "learning_rate": 4.1116437215689784e-05,
      "loss": 0.002,
      "step": 2770
    },
    {
      "epoch": 270.7317073170732,
      "grad_norm": 0.016656968742609024,
      "learning_rate": 4.1086396618344476e-05,
      "loss": 0.0017,
      "step": 2775
    },
    {
      "epoch": 271.219512195122,
      "grad_norm": 0.016044285148382187,
      "learning_rate": 4.1056316329414616e-05,
      "loss": 0.002,
      "step": 2780
    },
    {
      "epoch": 271.7073170731707,
      "grad_norm": 0.016289737075567245,
      "learning_rate": 4.102619642312031e-05,
      "loss": 0.0017,
      "step": 2785
    },
    {
      "epoch": 272.1951219512195,
      "grad_norm": 0.015751417726278305,
      "learning_rate": 4.0996036973779465e-05,
      "loss": 0.002,
      "step": 2790
    },
    {
      "epoch": 272.6829268292683,
      "grad_norm": 0.017929844558238983,
      "learning_rate": 4.0965838055807495e-05,
      "loss": 0.0017,
      "step": 2795
    },
    {
      "epoch": 273.1707317073171,
      "grad_norm": 0.01624852418899536,
      "learning_rate": 4.093559974371725e-05,
      "loss": 0.002,
      "step": 2800
    },
    {
      "epoch": 273.1707317073171,
      "eval_loss": 5.205387592315674,
      "eval_runtime": 2.0009,
      "eval_samples_per_second": 14.993,
      "eval_steps_per_second": 1.0,
      "step": 2800
    },
    {
      "epoch": 273.6585365853659,
      "grad_norm": 0.016230782493948936,
      "learning_rate": 4.090532211211874e-05,
      "loss": 0.0017,
      "step": 2805
    },
    {
      "epoch": 274.1463414634146,
      "grad_norm": 0.015834728255867958,
      "learning_rate": 4.087500523571902e-05,
      "loss": 0.0019,
      "step": 2810
    },
    {
      "epoch": 274.6341463414634,
      "grad_norm": 0.01574777439236641,
      "learning_rate": 4.084464918932197e-05,
      "loss": 0.0017,
      "step": 2815
    },
    {
      "epoch": 275.1219512195122,
      "grad_norm": 0.015806859359145164,
      "learning_rate": 4.0814254047828116e-05,
      "loss": 0.0019,
      "step": 2820
    },
    {
      "epoch": 275.609756097561,
      "grad_norm": 0.015415911562740803,
      "learning_rate": 4.0783819886234445e-05,
      "loss": 0.0017,
      "step": 2825
    },
    {
      "epoch": 276.0975609756098,
      "grad_norm": 0.01584634929895401,
      "learning_rate": 4.075334677963423e-05,
      "loss": 0.0019,
      "step": 2830
    },
    {
      "epoch": 276.5853658536585,
      "grad_norm": 0.015735110267996788,
      "learning_rate": 4.0722834803216836e-05,
      "loss": 0.0017,
      "step": 2835
    },
    {
      "epoch": 277.0731707317073,
      "grad_norm": 0.03907240554690361,
      "learning_rate": 4.0692284032267516e-05,
      "loss": 0.0019,
      "step": 2840
    },
    {
      "epoch": 277.5609756097561,
      "grad_norm": 0.016509829089045525,
      "learning_rate": 4.066169454216727e-05,
      "loss": 0.0016,
      "step": 2845
    },
    {
      "epoch": 278.0487804878049,
      "grad_norm": 0.0410178117454052,
      "learning_rate": 4.063106640839264e-05,
      "loss": 0.0019,
      "step": 2850
    },
    {
      "epoch": 278.5365853658537,
      "grad_norm": 0.014491982758045197,
      "learning_rate": 4.060039970651547e-05,
      "loss": 0.0016,
      "step": 2855
    },
    {
      "epoch": 279.0243902439024,
      "grad_norm": 0.044432107359170914,
      "learning_rate": 4.056969451220282e-05,
      "loss": 0.0019,
      "step": 2860
    },
    {
      "epoch": 279.5121951219512,
      "grad_norm": 0.015422770753502846,
      "learning_rate": 4.053895090121669e-05,
      "loss": 0.0016,
      "step": 2865
    },
    {
      "epoch": 280.0,
      "grad_norm": 0.03891604393720627,
      "learning_rate": 4.0508168949413906e-05,
      "loss": 0.0019,
      "step": 2870
    },
    {
      "epoch": 280.4878048780488,
      "grad_norm": 0.015515372157096863,
      "learning_rate": 4.047734873274586e-05,
      "loss": 0.0016,
      "step": 2875
    },
    {
      "epoch": 280.9756097560976,
      "grad_norm": 0.02206822857260704,
      "learning_rate": 4.044649032725836e-05,
      "loss": 0.0016,
      "step": 2880
    },
    {
      "epoch": 281.4634146341463,
      "grad_norm": 0.015314914286136627,
      "learning_rate": 4.0415593809091476e-05,
      "loss": 0.0018,
      "step": 2885
    },
    {
      "epoch": 281.9512195121951,
      "grad_norm": 0.01647641696035862,
      "learning_rate": 4.038465925447929e-05,
      "loss": 0.0016,
      "step": 2890
    },
    {
      "epoch": 282.4390243902439,
      "grad_norm": 0.014513181522488594,
      "learning_rate": 4.0353686739749734e-05,
      "loss": 0.0018,
      "step": 2895
    },
    {
      "epoch": 282.9268292682927,
      "grad_norm": 0.015378727577626705,
      "learning_rate": 4.0322676341324415e-05,
      "loss": 0.0016,
      "step": 2900
    },
    {
      "epoch": 282.9268292682927,
      "eval_loss": 5.244009971618652,
      "eval_runtime": 1.9943,
      "eval_samples_per_second": 15.043,
      "eval_steps_per_second": 1.003,
      "step": 2900
    },
    {
      "epoch": 283.4146341463415,
      "grad_norm": 0.01492574904114008,
      "learning_rate": 4.0291628135718404e-05,
      "loss": 0.0018,
      "step": 2905
    },
    {
      "epoch": 283.9024390243902,
      "grad_norm": 0.015621798112988472,
      "learning_rate": 4.0260542199540064e-05,
      "loss": 0.0016,
      "step": 2910
    },
    {
      "epoch": 284.390243902439,
      "grad_norm": 0.015359473414719105,
      "learning_rate": 4.022941860949085e-05,
      "loss": 0.0018,
      "step": 2915
    },
    {
      "epoch": 284.8780487804878,
      "grad_norm": 0.015298400074243546,
      "learning_rate": 4.019825744236514e-05,
      "loss": 0.0016,
      "step": 2920
    },
    {
      "epoch": 285.3658536585366,
      "grad_norm": 0.015204479917883873,
      "learning_rate": 4.0167058775049996e-05,
      "loss": 0.0018,
      "step": 2925
    },
    {
      "epoch": 285.8536585365854,
      "grad_norm": 0.014844435267150402,
      "learning_rate": 4.013582268452504e-05,
      "loss": 0.0015,
      "step": 2930
    },
    {
      "epoch": 286.3414634146341,
      "grad_norm": 0.013977671042084694,
      "learning_rate": 4.010454924786222e-05,
      "loss": 0.0018,
      "step": 2935
    },
    {
      "epoch": 286.8292682926829,
      "grad_norm": 0.013891782611608505,
      "learning_rate": 4.007323854222562e-05,
      "loss": 0.0015,
      "step": 2940
    },
    {
      "epoch": 287.3170731707317,
      "grad_norm": 0.015280971303582191,
      "learning_rate": 4.004189064487131e-05,
      "loss": 0.0018,
      "step": 2945
    },
    {
      "epoch": 287.8048780487805,
      "grad_norm": 0.014935857616364956,
      "learning_rate": 4.0010505633147106e-05,
      "loss": 0.0015,
      "step": 2950
    },
    {
      "epoch": 288.2926829268293,
      "grad_norm": 0.01509151142090559,
      "learning_rate": 3.99790835844924e-05,
      "loss": 0.0018,
      "step": 2955
    },
    {
      "epoch": 288.780487804878,
      "grad_norm": 0.015051239170134068,
      "learning_rate": 3.9947624576437975e-05,
      "loss": 0.0015,
      "step": 2960
    },
    {
      "epoch": 289.2682926829268,
      "grad_norm": 0.014529247768223286,
      "learning_rate": 3.9916128686605814e-05,
      "loss": 0.0018,
      "step": 2965
    },
    {
      "epoch": 289.7560975609756,
      "grad_norm": 0.015108738094568253,
      "learning_rate": 3.988459599270888e-05,
      "loss": 0.0015,
      "step": 2970
    },
    {
      "epoch": 290.2439024390244,
      "grad_norm": 0.014500374905765057,
      "learning_rate": 3.985302657255097e-05,
      "loss": 0.0017,
      "step": 2975
    },
    {
      "epoch": 290.7317073170732,
      "grad_norm": 0.014759327284991741,
      "learning_rate": 3.982142050402649e-05,
      "loss": 0.0015,
      "step": 2980
    },
    {
      "epoch": 291.219512195122,
      "grad_norm": 0.01463246438652277,
      "learning_rate": 3.978977786512026e-05,
      "loss": 0.0018,
      "step": 2985
    },
    {
      "epoch": 291.7073170731707,
      "grad_norm": 0.015961037948727608,
      "learning_rate": 3.975809873390737e-05,
      "loss": 0.0015,
      "step": 2990
    },
    {
      "epoch": 292.1951219512195,
      "grad_norm": 0.015271876938641071,
      "learning_rate": 3.972638318855291e-05,
      "loss": 0.0017,
      "step": 2995
    },
    {
      "epoch": 292.6829268292683,
      "grad_norm": 0.01602003164589405,
      "learning_rate": 3.969463130731183e-05,
      "loss": 0.0015,
      "step": 3000
    },
    {
      "epoch": 292.6829268292683,
      "eval_loss": 5.277629375457764,
      "eval_runtime": 2.0015,
      "eval_samples_per_second": 14.989,
      "eval_steps_per_second": 0.999,
      "step": 3000
    },
    {
      "epoch": 293.1707317073171,
      "grad_norm": 0.014217298477888107,
      "learning_rate": 3.966284316852876e-05,
      "loss": 0.0018,
      "step": 3005
    },
    {
      "epoch": 293.6585365853659,
      "grad_norm": 0.014695542864501476,
      "learning_rate": 3.963101885063776e-05,
      "loss": 0.0015,
      "step": 3010
    },
    {
      "epoch": 294.1463414634146,
      "grad_norm": 0.015018458478152752,
      "learning_rate": 3.959915843216216e-05,
      "loss": 0.0017,
      "step": 3015
    },
    {
      "epoch": 294.6341463414634,
      "grad_norm": 0.014443542808294296,
      "learning_rate": 3.9567261991714404e-05,
      "loss": 0.0015,
      "step": 3020
    },
    {
      "epoch": 295.1219512195122,
      "grad_norm": 0.013795099221169949,
      "learning_rate": 3.953532960799577e-05,
      "loss": 0.0017,
      "step": 3025
    },
    {
      "epoch": 295.609756097561,
      "grad_norm": 0.013609600253403187,
      "learning_rate": 3.950336135979624e-05,
      "loss": 0.0015,
      "step": 3030
    },
    {
      "epoch": 296.0975609756098,
      "grad_norm": 0.013391784392297268,
      "learning_rate": 3.947135732599428e-05,
      "loss": 0.0017,
      "step": 3035
    },
    {
      "epoch": 296.5853658536585,
      "grad_norm": 0.013999910093843937,
      "learning_rate": 3.943931758555669e-05,
      "loss": 0.0014,
      "step": 3040
    },
    {
      "epoch": 297.0731707317073,
      "grad_norm": 0.033058445900678635,
      "learning_rate": 3.940724221753832e-05,
      "loss": 0.0016,
      "step": 3045
    },
    {
      "epoch": 297.5609756097561,
      "grad_norm": 0.013800682500004768,
      "learning_rate": 3.937513130108197e-05,
      "loss": 0.0014,
      "step": 3050
    },
    {
      "epoch": 298.0487804878049,
      "grad_norm": 0.03725728765130043,
      "learning_rate": 3.9342984915418114e-05,
      "loss": 0.0016,
      "step": 3055
    },
    {
      "epoch": 298.5365853658537,
      "grad_norm": 0.013774747028946877,
      "learning_rate": 3.9310803139864775e-05,
      "loss": 0.0015,
      "step": 3060
    },
    {
      "epoch": 299.0243902439024,
      "grad_norm": 0.035114895552396774,
      "learning_rate": 3.927858605382728e-05,
      "loss": 0.0016,
      "step": 3065
    },
    {
      "epoch": 299.5121951219512,
      "grad_norm": 0.014101631008088589,
      "learning_rate": 3.9246333736798095e-05,
      "loss": 0.0014,
      "step": 3070
    },
    {
      "epoch": 300.0,
      "grad_norm": 0.04329006373882294,
      "learning_rate": 3.92140462683566e-05,
      "loss": 0.0016,
      "step": 3075
    },
    {
      "epoch": 300.4878048780488,
      "grad_norm": 0.013110330328345299,
      "learning_rate": 3.9181723728168916e-05,
      "loss": 0.0014,
      "step": 3080
    },
    {
      "epoch": 300.9756097560976,
      "grad_norm": 0.01297314465045929,
      "learning_rate": 3.914936619598769e-05,
      "loss": 0.0014,
      "step": 3085
    },
    {
      "epoch": 301.4634146341463,
      "grad_norm": 0.013401311822235584,
      "learning_rate": 3.911697375165193e-05,
      "loss": 0.0016,
      "step": 3090
    },
    {
      "epoch": 301.9512195121951,
      "grad_norm": 0.015091930516064167,
      "learning_rate": 3.908454647508676e-05,
      "loss": 0.0014,
      "step": 3095
    },
    {
      "epoch": 302.4390243902439,
      "grad_norm": 0.014082785695791245,
      "learning_rate": 3.905208444630327e-05,
      "loss": 0.0015,
      "step": 3100
    },
    {
      "epoch": 302.4390243902439,
      "eval_loss": 5.310764312744141,
      "eval_runtime": 2.1005,
      "eval_samples_per_second": 14.283,
      "eval_steps_per_second": 0.952,
      "step": 3100
    },
    {
      "epoch": 302.9268292682927,
      "grad_norm": 0.0137022128328681,
      "learning_rate": 3.9019587745398276e-05,
      "loss": 0.0014,
      "step": 3105
    },
    {
      "epoch": 303.4146341463415,
      "grad_norm": 0.012108751572668552,
      "learning_rate": 3.898705645255418e-05,
      "loss": 0.0016,
      "step": 3110
    },
    {
      "epoch": 303.9024390243902,
      "grad_norm": 0.013047149404883385,
      "learning_rate": 3.895449064803869e-05,
      "loss": 0.0014,
      "step": 3115
    },
    {
      "epoch": 304.390243902439,
      "grad_norm": 0.013208198361098766,
      "learning_rate": 3.8921890412204705e-05,
      "loss": 0.0016,
      "step": 3120
    },
    {
      "epoch": 304.8780487804878,
      "grad_norm": 0.013163234107196331,
      "learning_rate": 3.888925582549006e-05,
      "loss": 0.0014,
      "step": 3125
    },
    {
      "epoch": 305.3658536585366,
      "grad_norm": 0.013256290927529335,
      "learning_rate": 3.8856586968417354e-05,
      "loss": 0.0016,
      "step": 3130
    },
    {
      "epoch": 305.8536585365854,
      "grad_norm": 0.012773827649652958,
      "learning_rate": 3.8823883921593754e-05,
      "loss": 0.0014,
      "step": 3135
    },
    {
      "epoch": 306.3414634146341,
      "grad_norm": 0.01360262930393219,
      "learning_rate": 3.879114676571076e-05,
      "loss": 0.0016,
      "step": 3140
    },
    {
      "epoch": 306.8292682926829,
      "grad_norm": 0.013681067153811455,
      "learning_rate": 3.875837558154406e-05,
      "loss": 0.0013,
      "step": 3145
    },
    {
      "epoch": 307.3170731707317,
      "grad_norm": 0.012061741203069687,
      "learning_rate": 3.87255704499533e-05,
      "loss": 0.0015,
      "step": 3150
    },
    {
      "epoch": 307.8048780487805,
      "grad_norm": 0.013421589508652687,
      "learning_rate": 3.869273145188187e-05,
      "loss": 0.0013,
      "step": 3155
    },
    {
      "epoch": 308.2926829268293,
      "grad_norm": 0.012535752728581429,
      "learning_rate": 3.865985866835673e-05,
      "loss": 0.0015,
      "step": 3160
    },
    {
      "epoch": 308.780487804878,
      "grad_norm": 0.013780146837234497,
      "learning_rate": 3.8626952180488216e-05,
      "loss": 0.0013,
      "step": 3165
    },
    {
      "epoch": 309.2682926829268,
      "grad_norm": 0.013017554767429829,
      "learning_rate": 3.859401206946982e-05,
      "loss": 0.0015,
      "step": 3170
    },
    {
      "epoch": 309.7560975609756,
      "grad_norm": 0.013191579841077328,
      "learning_rate": 3.856103841657797e-05,
      "loss": 0.0013,
      "step": 3175
    },
    {
      "epoch": 310.2439024390244,
      "grad_norm": 0.012843555770814419,
      "learning_rate": 3.8528031303171895e-05,
      "loss": 0.0015,
      "step": 3180
    },
    {
      "epoch": 310.7317073170732,
      "grad_norm": 0.013041529804468155,
      "learning_rate": 3.8494990810693366e-05,
      "loss": 0.0013,
      "step": 3185
    },
    {
      "epoch": 311.219512195122,
      "grad_norm": 0.013854299671947956,
      "learning_rate": 3.8461917020666504e-05,
      "loss": 0.0015,
      "step": 3190
    },
    {
      "epoch": 311.7073170731707,
      "grad_norm": 0.012789745815098286,
      "learning_rate": 3.8428810014697615e-05,
      "loss": 0.0013,
      "step": 3195
    },
    {
      "epoch": 312.1951219512195,
      "grad_norm": 0.012485920451581478,
      "learning_rate": 3.8395669874474915e-05,
      "loss": 0.0015,
      "step": 3200
    },
    {
      "epoch": 312.1951219512195,
      "eval_loss": 5.345956802368164,
      "eval_runtime": 2.0839,
      "eval_samples_per_second": 14.396,
      "eval_steps_per_second": 0.96,
      "step": 3200
    },
    {
      "epoch": 312.6829268292683,
      "grad_norm": 0.014380828477442265,
      "learning_rate": 3.836249668176844e-05,
      "loss": 0.0013,
      "step": 3205
    },
    {
      "epoch": 313.1707317073171,
      "grad_norm": 0.012757360935211182,
      "learning_rate": 3.832929051842972e-05,
      "loss": 0.0015,
      "step": 3210
    },
    {
      "epoch": 313.6585365853659,
      "grad_norm": 0.012495512142777443,
      "learning_rate": 3.829605146639167e-05,
      "loss": 0.0013,
      "step": 3215
    },
    {
      "epoch": 314.1463414634146,
      "grad_norm": 0.012485985644161701,
      "learning_rate": 3.826277960766835e-05,
      "loss": 0.0014,
      "step": 3220
    },
    {
      "epoch": 314.6341463414634,
      "grad_norm": 0.012684849090874195,
      "learning_rate": 3.822947502435477e-05,
      "loss": 0.0013,
      "step": 3225
    },
    {
      "epoch": 315.1219512195122,
      "grad_norm": 0.011646721512079239,
      "learning_rate": 3.819613779862666e-05,
      "loss": 0.0015,
      "step": 3230
    },
    {
      "epoch": 315.609756097561,
      "grad_norm": 0.012041942216455936,
      "learning_rate": 3.816276801274032e-05,
      "loss": 0.0013,
      "step": 3235
    },
    {
      "epoch": 316.0975609756098,
      "grad_norm": 0.01242943573743105,
      "learning_rate": 3.81293657490324e-05,
      "loss": 0.0015,
      "step": 3240
    },
    {
      "epoch": 316.5853658536585,
      "grad_norm": 0.012053756043314934,
      "learning_rate": 3.809593108991962e-05,
      "loss": 0.0013,
      "step": 3245
    },
    {
      "epoch": 317.0731707317073,
      "grad_norm": 0.030526988208293915,
      "learning_rate": 3.8062464117898724e-05,
      "loss": 0.0015,
      "step": 3250
    },
    {
      "epoch": 317.5609756097561,
      "grad_norm": 0.011758929118514061,
      "learning_rate": 3.802896491554611e-05,
      "loss": 0.0012,
      "step": 3255
    },
    {
      "epoch": 318.0487804878049,
      "grad_norm": 0.029359810054302216,
      "learning_rate": 3.7995433565517735e-05,
      "loss": 0.0015,
      "step": 3260
    },
    {
      "epoch": 318.5365853658537,
      "grad_norm": 0.011844493448734283,
      "learning_rate": 3.796187015054888e-05,
      "loss": 0.0012,
      "step": 3265
    },
    {
      "epoch": 319.0243902439024,
      "grad_norm": 0.033759575337171555,
      "learning_rate": 3.792827475345393e-05,
      "loss": 0.0014,
      "step": 3270
    },
    {
      "epoch": 319.5121951219512,
      "grad_norm": 0.012137740850448608,
      "learning_rate": 3.789464745712619e-05,
      "loss": 0.0013,
      "step": 3275
    },
    {
      "epoch": 320.0,
      "grad_norm": 0.028825733810663223,
      "learning_rate": 3.786098834453766e-05,
      "loss": 0.0014,
      "step": 3280
    },
    {
      "epoch": 320.4878048780488,
      "grad_norm": 0.012119987979531288,
      "learning_rate": 3.7827297498738876e-05,
      "loss": 0.0012,
      "step": 3285
    },
    {
      "epoch": 320.9756097560976,
      "grad_norm": 0.01211267989128828,
      "learning_rate": 3.779357500285863e-05,
      "loss": 0.0012,
      "step": 3290
    },
    {
      "epoch": 321.4634146341463,
      "grad_norm": 0.011430900543928146,
      "learning_rate": 3.775982094010383e-05,
      "loss": 0.0014,
      "step": 3295
    },
    {
      "epoch": 321.9512195121951,
      "grad_norm": 0.01223862823098898,
      "learning_rate": 3.7726035393759285e-05,
      "loss": 0.0012,
      "step": 3300
    },
    {
      "epoch": 321.9512195121951,
      "eval_loss": 5.386119365692139,
      "eval_runtime": 1.9999,
      "eval_samples_per_second": 15.001,
      "eval_steps_per_second": 1.0,
      "step": 3300
    },
    {
      "epoch": 322.4390243902439,
      "grad_norm": 0.011707975529134274,
      "learning_rate": 3.769221844718746e-05,
      "loss": 0.0014,
      "step": 3305
    },
    {
      "epoch": 322.9268292682927,
      "grad_norm": 0.011983509175479412,
      "learning_rate": 3.765837018382831e-05,
      "loss": 0.0012,
      "step": 3310
    },
    {
      "epoch": 323.4146341463415,
      "grad_norm": 0.011713515035808086,
      "learning_rate": 3.762449068719907e-05,
      "loss": 0.0014,
      "step": 3315
    },
    {
      "epoch": 323.9024390243902,
      "grad_norm": 0.011494801379740238,
      "learning_rate": 3.759058004089402e-05,
      "loss": 0.0012,
      "step": 3320
    },
    {
      "epoch": 324.390243902439,
      "grad_norm": 0.011180885136127472,
      "learning_rate": 3.755663832858432e-05,
      "loss": 0.0014,
      "step": 3325
    },
    {
      "epoch": 324.8780487804878,
      "grad_norm": 0.011459407396614552,
      "learning_rate": 3.752266563401775e-05,
      "loss": 0.0012,
      "step": 3330
    },
    {
      "epoch": 325.3658536585366,
      "grad_norm": 0.011838763020932674,
      "learning_rate": 3.7488662041018575e-05,
      "loss": 0.0014,
      "step": 3335
    },
    {
      "epoch": 325.8536585365854,
      "grad_norm": 0.012045239098370075,
      "learning_rate": 3.7454627633487274e-05,
      "loss": 0.0012,
      "step": 3340
    },
    {
      "epoch": 326.3414634146341,
      "grad_norm": 0.011636377312242985,
      "learning_rate": 3.742056249540036e-05,
      "loss": 0.0014,
      "step": 3345
    },
    {
      "epoch": 326.8292682926829,
      "grad_norm": 0.012123490683734417,
      "learning_rate": 3.7386466710810194e-05,
      "loss": 0.0012,
      "step": 3350
    },
    {
      "epoch": 327.3170731707317,
      "grad_norm": 0.010970481671392918,
      "learning_rate": 3.7352340363844704e-05,
      "loss": 0.0013,
      "step": 3355
    },
    {
      "epoch": 327.8048780487805,
      "grad_norm": 0.012314393185079098,
      "learning_rate": 3.731818353870729e-05,
      "loss": 0.0012,
      "step": 3360
    },
    {
      "epoch": 328.2926829268293,
      "grad_norm": 0.01159825548529625,
      "learning_rate": 3.728399631967651e-05,
      "loss": 0.0013,
      "step": 3365
    },
    {
      "epoch": 328.780487804878,
      "grad_norm": 0.011910730041563511,
      "learning_rate": 3.724977879110591e-05,
      "loss": 0.0012,
      "step": 3370
    },
    {
      "epoch": 329.2682926829268,
      "grad_norm": 0.01144592184573412,
      "learning_rate": 3.721553103742388e-05,
      "loss": 0.0014,
      "step": 3375
    },
    {
      "epoch": 329.7560975609756,
      "grad_norm": 0.011673355475068092,
      "learning_rate": 3.718125314313331e-05,
      "loss": 0.0011,
      "step": 3380
    },
    {
      "epoch": 330.2439024390244,
      "grad_norm": 0.011107510887086391,
      "learning_rate": 3.714694519281152e-05,
      "loss": 0.0013,
      "step": 3385
    },
    {
      "epoch": 330.7317073170732,
      "grad_norm": 0.011226373724639416,
      "learning_rate": 3.711260727110995e-05,
      "loss": 0.0012,
      "step": 3390
    },
    {
      "epoch": 331.219512195122,
      "grad_norm": 0.010969772934913635,
      "learning_rate": 3.707823946275402e-05,
      "loss": 0.0013,
      "step": 3395
    },
    {
      "epoch": 331.7073170731707,
      "grad_norm": 0.011724336072802544,
      "learning_rate": 3.704384185254288e-05,
      "loss": 0.0011,
      "step": 3400
    },
    {
      "epoch": 331.7073170731707,
      "eval_loss": 5.391245365142822,
      "eval_runtime": 2.0005,
      "eval_samples_per_second": 14.996,
      "eval_steps_per_second": 1.0,
      "step": 3400
    },
    {
      "epoch": 332.1951219512195,
      "grad_norm": 0.011212178505957127,
      "learning_rate": 3.700941452534922e-05,
      "loss": 0.0013,
      "step": 3405
    },
    {
      "epoch": 332.6829268292683,
      "grad_norm": 0.011458317749202251,
      "learning_rate": 3.697495756611903e-05,
      "loss": 0.0011,
      "step": 3410
    },
    {
      "epoch": 333.1707317073171,
      "grad_norm": 0.011183385737240314,
      "learning_rate": 3.694047105987144e-05,
      "loss": 0.0013,
      "step": 3415
    },
    {
      "epoch": 333.6585365853659,
      "grad_norm": 0.010991326533257961,
      "learning_rate": 3.690595509169848e-05,
      "loss": 0.0011,
      "step": 3420
    },
    {
      "epoch": 334.1463414634146,
      "grad_norm": 0.011439093388617039,
      "learning_rate": 3.6871409746764865e-05,
      "loss": 0.0013,
      "step": 3425
    },
    {
      "epoch": 334.6341463414634,
      "grad_norm": 0.011279362253844738,
      "learning_rate": 3.68368351103078e-05,
      "loss": 0.0011,
      "step": 3430
    },
    {
      "epoch": 335.1219512195122,
      "grad_norm": 0.010888714343309402,
      "learning_rate": 3.680223126763677e-05,
      "loss": 0.0013,
      "step": 3435
    },
    {
      "epoch": 335.609756097561,
      "grad_norm": 0.011779086664319038,
      "learning_rate": 3.6767598304133324e-05,
      "loss": 0.0011,
      "step": 3440
    },
    {
      "epoch": 336.0975609756098,
      "grad_norm": 0.010450751520693302,
      "learning_rate": 3.673293630525083e-05,
      "loss": 0.0013,
      "step": 3445
    },
    {
      "epoch": 336.5853658536585,
      "grad_norm": 0.011833149008452892,
      "learning_rate": 3.6698245356514335e-05,
      "loss": 0.0011,
      "step": 3450
    },
    {
      "epoch": 337.0731707317073,
      "grad_norm": 0.027948416769504547,
      "learning_rate": 3.666352554352032e-05,
      "loss": 0.0013,
      "step": 3455
    },
    {
      "epoch": 337.5609756097561,
      "grad_norm": 0.010566859506070614,
      "learning_rate": 3.662877695193646e-05,
      "loss": 0.0011,
      "step": 3460
    },
    {
      "epoch": 338.0487804878049,
      "grad_norm": 0.02685127966105938,
      "learning_rate": 3.6593999667501454e-05,
      "loss": 0.0013,
      "step": 3465
    },
    {
      "epoch": 338.5365853658537,
      "grad_norm": 0.010240311734378338,
      "learning_rate": 3.6559193776024794e-05,
      "loss": 0.0011,
      "step": 3470
    },
    {
      "epoch": 339.0243902439024,
      "grad_norm": 0.031611938029527664,
      "learning_rate": 3.652435936338656e-05,
      "loss": 0.0013,
      "step": 3475
    },
    {
      "epoch": 339.5121951219512,
      "grad_norm": 0.012756749987602234,
      "learning_rate": 3.6489496515537204e-05,
      "loss": 0.0011,
      "step": 3480
    },
    {
      "epoch": 340.0,
      "grad_norm": 0.026971803978085518,
      "learning_rate": 3.6454605318497326e-05,
      "loss": 0.0013,
      "step": 3485
    },
    {
      "epoch": 340.4878048780488,
      "grad_norm": 0.011205416172742844,
      "learning_rate": 3.641968585835749e-05,
      "loss": 0.0011,
      "step": 3490
    },
    {
      "epoch": 340.9756097560976,
      "grad_norm": 0.01210436038672924,
      "learning_rate": 3.6384738221278e-05,
      "loss": 0.0011,
      "step": 3495
    },
    {
      "epoch": 341.4634146341463,
      "grad_norm": 0.010852843523025513,
      "learning_rate": 3.634976249348867e-05,
      "loss": 0.0012,
      "step": 3500
    },
    {
      "epoch": 341.4634146341463,
      "eval_loss": 5.445451259613037,
      "eval_runtime": 2.001,
      "eval_samples_per_second": 14.993,
      "eval_steps_per_second": 1.0,
      "step": 3500
    },
    {
      "epoch": 341.9512195121951,
      "grad_norm": 0.01118863932788372,
      "learning_rate": 3.631475876128864e-05,
      "loss": 0.0011,
      "step": 3505
    },
    {
      "epoch": 342.4390243902439,
      "grad_norm": 0.01097920909523964,
      "learning_rate": 3.627972711104613e-05,
      "loss": 0.0012,
      "step": 3510
    },
    {
      "epoch": 342.9268292682927,
      "grad_norm": 0.010851696133613586,
      "learning_rate": 3.624466762919826e-05,
      "loss": 0.0011,
      "step": 3515
    },
    {
      "epoch": 343.4146341463415,
      "grad_norm": 0.010194485075771809,
      "learning_rate": 3.6209580402250815e-05,
      "loss": 0.0013,
      "step": 3520
    },
    {
      "epoch": 343.9024390243902,
      "grad_norm": 0.010732091031968594,
      "learning_rate": 3.6174465516778035e-05,
      "loss": 0.0011,
      "step": 3525
    },
    {
      "epoch": 344.390243902439,
      "grad_norm": 0.01059865951538086,
      "learning_rate": 3.6139323059422415e-05,
      "loss": 0.0012,
      "step": 3530
    },
    {
      "epoch": 344.8780487804878,
      "grad_norm": 0.010712736286222935,
      "learning_rate": 3.610415311689447e-05,
      "loss": 0.0011,
      "step": 3535
    },
    {
      "epoch": 345.3658536585366,
      "grad_norm": 0.01020485907793045,
      "learning_rate": 3.606895577597255e-05,
      "loss": 0.0012,
      "step": 3540
    },
    {
      "epoch": 345.8536585365854,
      "grad_norm": 0.010447642765939236,
      "learning_rate": 3.6033731123502566e-05,
      "loss": 0.001,
      "step": 3545
    },
    {
      "epoch": 346.3414634146341,
      "grad_norm": 0.01040102168917656,
      "learning_rate": 3.599847924639788e-05,
      "loss": 0.0012,
      "step": 3550
    },
    {
      "epoch": 346.8292682926829,
      "grad_norm": 0.010066005401313305,
      "learning_rate": 3.5963200231638976e-05,
      "loss": 0.001,
      "step": 3555
    },
    {
      "epoch": 347.3170731707317,
      "grad_norm": 0.010544953867793083,
      "learning_rate": 3.592789416627332e-05,
      "loss": 0.0012,
      "step": 3560
    },
    {
      "epoch": 347.8048780487805,
      "grad_norm": 0.011079319752752781,
      "learning_rate": 3.589256113741513e-05,
      "loss": 0.0011,
      "step": 3565
    },
    {
      "epoch": 348.2926829268293,
      "grad_norm": 0.010934559628367424,
      "learning_rate": 3.585720123224512e-05,
      "loss": 0.0012,
      "step": 3570
    },
    {
      "epoch": 348.780487804878,
      "grad_norm": 0.012050378136336803,
      "learning_rate": 3.582181453801036e-05,
      "loss": 0.001,
      "step": 3575
    },
    {
      "epoch": 349.2682926829268,
      "grad_norm": 0.009861963801085949,
      "learning_rate": 3.5786401142023975e-05,
      "loss": 0.0012,
      "step": 3580
    },
    {
      "epoch": 349.7560975609756,
      "grad_norm": 0.010045533068478107,
      "learning_rate": 3.5750961131665034e-05,
      "loss": 0.001,
      "step": 3585
    },
    {
      "epoch": 350.2439024390244,
      "grad_norm": 0.010450680740177631,
      "learning_rate": 3.5715494594378216e-05,
      "loss": 0.0011,
      "step": 3590
    },
    {
      "epoch": 350.7317073170732,
      "grad_norm": 0.01055144052952528,
      "learning_rate": 3.568000161767368e-05,
      "loss": 0.001,
      "step": 3595
    },
    {
      "epoch": 351.219512195122,
      "grad_norm": 0.010520314797759056,
      "learning_rate": 3.564448228912682e-05,
      "loss": 0.0012,
      "step": 3600
    },
    {
      "epoch": 351.219512195122,
      "eval_loss": 5.453519344329834,
      "eval_runtime": 2.0836,
      "eval_samples_per_second": 14.398,
      "eval_steps_per_second": 0.96,
      "step": 3600
    },
    {
      "epoch": 351.7073170731707,
      "grad_norm": 0.010075603611767292,
      "learning_rate": 3.560893669637805e-05,
      "loss": 0.001,
      "step": 3605
    },
    {
      "epoch": 352.1951219512195,
      "grad_norm": 0.011497790925204754,
      "learning_rate": 3.557336492713258e-05,
      "loss": 0.0012,
      "step": 3610
    },
    {
      "epoch": 352.6829268292683,
      "grad_norm": 0.010038741864264011,
      "learning_rate": 3.5537767069160234e-05,
      "loss": 0.001,
      "step": 3615
    },
    {
      "epoch": 353.1707317073171,
      "grad_norm": 0.010953525081276894,
      "learning_rate": 3.5502143210295165e-05,
      "loss": 0.0012,
      "step": 3620
    },
    {
      "epoch": 353.6585365853659,
      "grad_norm": 0.010261130519211292,
      "learning_rate": 3.54664934384357e-05,
      "loss": 0.001,
      "step": 3625
    },
    {
      "epoch": 354.1463414634146,
      "grad_norm": 0.009617283940315247,
      "learning_rate": 3.543081784154414e-05,
      "loss": 0.0012,
      "step": 3630
    },
    {
      "epoch": 354.6341463414634,
      "grad_norm": 0.009909265674650669,
      "learning_rate": 3.5395116507646435e-05,
      "loss": 0.001,
      "step": 3635
    },
    {
      "epoch": 355.1219512195122,
      "grad_norm": 0.010044896975159645,
      "learning_rate": 3.535938952483211e-05,
      "loss": 0.0012,
      "step": 3640
    },
    {
      "epoch": 355.609756097561,
      "grad_norm": 0.00991596095263958,
      "learning_rate": 3.532363698125392e-05,
      "loss": 0.001,
      "step": 3645
    },
    {
      "epoch": 356.0975609756098,
      "grad_norm": 0.009530908428132534,
      "learning_rate": 3.528785896512772e-05,
      "loss": 0.0011,
      "step": 3650
    },
    {
      "epoch": 356.5853658536585,
      "grad_norm": 0.009094953536987305,
      "learning_rate": 3.525205556473221e-05,
      "loss": 0.001,
      "step": 3655
    },
    {
      "epoch": 357.0731707317073,
      "grad_norm": 0.026425521820783615,
      "learning_rate": 3.521622686840873e-05,
      "loss": 0.0011,
      "step": 3660
    },
    {
      "epoch": 357.5609756097561,
      "grad_norm": 0.009971903637051582,
      "learning_rate": 3.5180372964561015e-05,
      "loss": 0.001,
      "step": 3665
    },
    {
      "epoch": 358.0487804878049,
      "grad_norm": 0.023531083017587662,
      "learning_rate": 3.5144493941655e-05,
      "loss": 0.0011,
      "step": 3670
    },
    {
      "epoch": 358.5365853658537,
      "grad_norm": 0.009831216186285019,
      "learning_rate": 3.510858988821863e-05,
      "loss": 0.001,
      "step": 3675
    },
    {
      "epoch": 359.0243902439024,
      "grad_norm": 0.025496231392025948,
      "learning_rate": 3.507266089284157e-05,
      "loss": 0.0011,
      "step": 3680
    },
    {
      "epoch": 359.5121951219512,
      "grad_norm": 0.009538466110825539,
      "learning_rate": 3.5036707044175054e-05,
      "loss": 0.001,
      "step": 3685
    },
    {
      "epoch": 360.0,
      "grad_norm": 0.025208313018083572,
      "learning_rate": 3.500072843093162e-05,
      "loss": 0.0011,
      "step": 3690
    },
    {
      "epoch": 360.4878048780488,
      "grad_norm": 0.01020805537700653,
      "learning_rate": 3.496472514188493e-05,
      "loss": 0.001,
      "step": 3695
    },
    {
      "epoch": 360.9756097560976,
      "grad_norm": 0.011507493443787098,
      "learning_rate": 3.4928697265869515e-05,
      "loss": 0.001,
      "step": 3700
    },
    {
      "epoch": 360.9756097560976,
      "eval_loss": 5.494840621948242,
      "eval_runtime": 2.0903,
      "eval_samples_per_second": 14.352,
      "eval_steps_per_second": 0.957,
      "step": 3700
    }
  ],
  "logging_steps": 5,
  "max_steps": 10000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1000,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.056412704308593e+17,
  "train_batch_size": 18,
  "trial_name": null,
  "trial_params": null
}
